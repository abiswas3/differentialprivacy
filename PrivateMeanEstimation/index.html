<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../blog.css">
  <script src="../code/es5/tex-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/code/bootstrap.min.css">
  <script src="/code/jquery.min.js"></script>
  <script src="/code/bootstrap.min.js"></script>
  
  
  <ul class="bar">
    <li class="barli"><a class="active" href="/">Home</a></li>
    <li class="barli"><a href="/differentialprivacy/index.html">Notes on Differential Privacy</a></li>  
    <!-- <li class="barli"><a href="https://abiswas3.github.io/differentialprivacy.github.io">Notes on Differential Privacy</a></li> -->
  </ul>
</head>
<body>

<div class="container">
<h1 id="private-mean-estimation-a-survey">Private Mean Estimation – A survey</h1>
<p>Papers being sumarised:</p>
<ul>
<li><a href="">Laplace</a></li>
<li><a href="">Graham survey</a></li>
<li><a href="">Cheu I</a></li>
<li><a href="">Gazi I</a></li>
<li><a href="">Gazi II</a></li>
</ul>
<h2 id="problem-statement">Problem Statement</h2>
<p><code>N users have values in the bounded reals or bounded integers or binary numbers. We want to calculate the mean/sum of these numbers privately. What have people done so far towards this problem. What assumptions have people made to make this problem morre private or more accurate.</code></p>
<p>The table below provides an overview of the understanding of the summing problem we have today. There are three modes of privacy - central, local and a hybrid of the two known as shuffle. Each paradigm has a slighlty different definition of differential privacy. The list of definitions is <a href="../Definitions/">provided here</a></p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-lt5t{background-color:#ecf4ff;border-color:inherit;color:#333333;text-align:left;vertical-align:top}
.tg .tg-hf8k{background-color:#ecf4ff;color:#333333;text-align:left;vertical-align:top}
.tg .tg-item{background-color:#00d2cb;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-ltxa{background-color:#ffccc9;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-90e1{background-color:#ffccc9;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0khl{background-color:#00d2cb;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
<tr>
<th class="tg-0pky">
</th>
<th class="tg-0lax">
</th>
<th class="tg-fymr">
Upper Bound
</th>
<th class="tg-fymr">
Number of messages per user
</th>
<th class="tg-0pky">
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-lt5t">
Central
</td>
<td class="tg-hf8k">
</td>
<td class="tg-lt5t">
<a href="../Definitions/"><span class="math inline">\(O(\frac{1}{\epsilon})\)</span></a>
</td>
<td class="tg-lt5t">
<span style="font-weight:400;font-style:normal">1</span>
</td>
<td class="tg-lt5t">
</td>
</tr>
<tr>
<td class="tg-90e1">
Local
</td>
<td class="tg-ltxa">
</td>
<td class="tg-90e1">
<a href="https://arxiv.org/pdf/1103.2626.pdf" target="_blank" rel="noopener noreferrer"><span style="color:#905"><a href="https://arxiv.org/pdf/1103.2626.pdf"><span class="math inline">\(O(\frac{1}{\sqrt{n}})\)</span></a></span></a><br>
</td>
<td class="tg-90e1">
1
</td>
<td class="tg-90e1">
</td>
</tr>
<tr>
<td class="tg-item">
Shuffle
</td>
<td class="tg-0khl">
Privacy Blanket - Borja
</td>
<td class="tg-item">
<span style="font-weight:400;font-style:normal"><span class="math inline">\(O(\frac{1}{\epsilon})\)</span></span>
</td>
<td class="tg-item">
<span class="math inline">\(O_{\epsilon}\Big( \log\frac{n}{\delta}\Big)\)</span>
</td>
<td class="tg-item">
</td>
</tr>
<tr>
<td class="tg-0khl">
</td>
<td class="tg-0khl">
<span style="font-weight:400;font-style:normal">Vanishing complexity - Ghazi,</span><br><span style="font-weight:400;font-style:normal">(for binary sums)</span><br>
</td>
<td class="tg-0khl">
<span style="font-weight:400;font-style:normal"><span class="math inline">\(O(\frac{1}{\epsilon})\)</span></span>
</td>
<td class="tg-0khl">
<span style="font-weight:400;font-style:normal"><span class="math inline">\(O_{\epsilon}\Big( \frac{\log^2\frac{1}{\delta}}{n}\Big)\)</span></span>
</td>
<td class="tg-0khl">
</td>
</tr>
<tr>
<td class="tg-0khl">
</td>
<td class="tg-0khl">
<span style="font-weight:400;font-style:normal">Vanishing complexity - Ghazi,</span><br><span style="font-weight:400;font-style:normal">(for binary sums)</span>
</td>
<td class="tg-0khl">
<span class="math inline">\(\Omega\Big( \sqrt{\log \frac{1}{\delta}}\Big)\)</span>
</td>
<td class="tg-0khl">
1
</td>
<td class="tg-0khl">
</td>
</tr>
<tr>
<td class="tg-0khl">
</td>
<td class="tg-0khl">
Almost Central
</td>
<td class="tg-0khl">
</td>
<td class="tg-0khl">
</td>
<td class="tg-0khl">
</td>
</tr>
</tbody>
</table>
<blockquote>

</blockquote>
<h2 id="central-privacy">Central Privacy</h2>
<h3 id="for-real-numbers">For real numbers</h3>
<p>We incur a probability</p>
<h3 id="for-integers">For integers</h3>
<p>Since the input values are integers or binary numbers, we cannot use noise from the continuous laplace distribution. Instead we use its discrete counter part - the two sided geometric distribution or the discrete laplace distribution. [<a href="../DiscreteLaplace/paper.pdf" title="UNIVERSALLY UTILITY-MAXIMIZING PRIVACY MECHANISMS">1</a>] showed that this mechanism was optimum i.e. the error of this mechanism is a lower bound for central differential privacy. We cannot do better.</p>
<h2 id="local-privacy">Local Privacy</h2>
<h3 id="binary-sums-with-randomised-response">Binary sums with Randomised Response</h3>
<p>We describe the randomised response protocol next.</p>
<div class="algorithm">
<p><strong>Local Randomiser</strong></p>
<p>Input:</p>
<ul>
<li><span class="math inline">\(x_i \in \{ 0, 1 \}\)</span></li>
<li><span class="math inline">\(\gamma \in (0,1/2)\)</span></li>
</ul>
<p>Method:</p>
<ul>
<li>Flip a coin with probability of heads = <span class="math inline">\(0.5 + \gamma\)</span>.</li>
<li>If heads return <span class="math inline">\(z_i = x_i\)</span> else return <span class="math inline">\(z_i=1 - x_i\)</span></li>
</ul>
<p><strong>Analyser</strong></p>
<ul>
<li><p>Compute <span class="math inline">\(k = (\sum_{i}^N z_i)\)</span></p></li>
<li><p>Output <span class="math inline">\(\hat{f} = \frac{ k- (0.5 - \gamma)N}{2\gamma}\)</span></p></li>
</ul>
</div>
<h3 id="privacy">Privacy</h3>
<p>Privacy for the above algorithm is trivially achieved <strong>(will insert it later)</strong></p>
<h4 id="upper-bound">Upper Bound</h4>
<p>We now try to upper bound the accuracy of the algorithm discussed above.</p>
<button type="button" class="btn btn-info" data-toggle="collapse" data-target="#randomResponseError">
Proof
</button>
<div id="randomResponseError" class="collapse">
<p>The expected value of <span class="math inline">\(z_i = 0.5 + \gamma\)</span> when <span class="math inline">\(x_i=1\)</span> and <span class="math inline">\(0.5-\gamma\)</span> when <span class="math inline">\(x_i=0\)</span>. Thus</p>
<span class="math display">\[\begin{align*}
\mathbb{E}[\hat{f}] &amp;= \mathbb{E}[\frac{ k- (0.5 - \gamma)N}{2\gamma}] \\
&amp;= \frac{1}{2\gamma}\Big(\mathbb{E}[k] -  0.5N - N\gamma \Big) \\
&amp;= \frac{1}{2\gamma}\Big(\mathbb{E}\Big[(0.5 + \gamma)k + (0.5 - \gamma)(N -k) \Big] -  0.5N - N\gamma \Big) \\
&amp;= k
\end{align*}\]</span>
</div>
<p>Thus <span class="math inline">\(\hat{f}\)</span> is an unbiased estimator of the sum of inputs, and since <span class="math inline">\(\hat{f}\)</span> is a sum of bernoulli random variables, it means we can directly apply the <a href="http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Jan29_Tudor.pdf">hoefdinng bound for Bernoulli’s in Example 1</a> which states</p>
<p><span class="math display">\[ \mathbb{P}\Big[ |\hat{f} - \mathbb{E}(\hat{f})| \geq t \Big] \leq 2e^{-2nt^2}\]</span></p>
<p>To pull out an error bound set <span class="math inline">\(\delta = 2e^{-2nt^2}\)</span> and solve for t. Thus we get with probability <span class="math inline">\(1 - \delta\)</span>, <span class="math inline">\(|\hat{f} - \mathbb{E}(\hat{f})| \leq t\)</span> and <span class="math inline">\(t=\sqrt{\frac{1}{2n}\log\frac{1}{\delta}}\)</span>. Thus we can finally say that with probability <span class="math inline">\(1 - \delta\)</span></p>
<p><span class="math display">\[|\hat{f} - \mathbb{E}(\hat{f})| \leq O(\frac{1}{\sqrt{N}})\]</span></p>
<p>An alternate way to derive this bound is to look at the variance of the estimator <span class="math inline">\(\hat{f}\)</span> and since the estimator is unbiased, the variance of the estimator is equal to the mean squared error of the estimator. It gives a point estimate of an error not a confidence interval. The derivation can be found <a href="http://www.gautamkamath.com/CS860notes/lec3.pdf">here</a>. It is relatively straightforward.</p>
<h4 id="lower-bound">Lower Bound</h4>
[<a href="https://arxiv.org/pdf/1103.2626.pdf" title="Distributed Private Data Analysis: On Simultaneously Solving How and What">3</a>] Gives us a proof that shows that the simple algorithm above that was invented in 1965 is actually the best you can do. It states that the upper bound is tight – that indeed <span class="math inline">\(O(\frac{1}{\sqrt{N}})\)</span> is the least error we must tolerate to ensure privacy.
<div class="question">
The proof for this lower bound is quite convoluted. The authors prove the lower bound for any distributed t-coalition network, for interactive and non interactive protocols for sums. I try and simplify their proof for non interactive local privacy only. In doing so, I do my best attempt at simplifying their proof, building intuition and re-deriving it.
</div>
<p>What a lower bound statement is saying is that – if we wanted to <span class="math inline">\(\epsilon\)</span> local privacy and compute the mean of binary numbers, <strong>all</strong> private algorithms will error at least by <span class="math inline">\(O(\frac{1}{\sqrt{N}})\)</span> with non zero probability and non vanishing probability (the probability does not shrink as number of users go up.)</p>
<button type="button" class="btn btn-info" data-toggle="collapse" data-target="#lowerBoundLocalMean">
Lower bound for binary mean estimation
</button>
<div id="lowerBoundLocalMean" class="collapse">
<p>The proof is divided into 2 parts:</p>
<ol type="1">
<li><p>They first define a gap version of the threshold function, denoted GAP-TR, and observe that any differentially private protocol for SUM with error <span class="math inline">\(\tau\)</span> implies a differentially-private protocol for GAP-TR with gap <span class="math inline">\(\frac{\tau}{2}\)</span>.</p></li>
<li><p>The contrapositive of the above fact, is used to show that it is impossible to compute GAP-TR with a gap smaller than <span class="math inline">\(O(\sqrt{n})\)</span> in a differentially private manner. Therefore, it is impossible to compute SUM in a differentially private manner without suffering an error of at least <span class="math inline">\(O(\sqrt{n})\)</span>.</p></li>
</ol>
<p>Most of the work in the paper is done to prove the second bit.</p>
<p><strong><span class="math inline">\(GAP-TR_{\kappa, \tau}\)</span></strong> Gap threshold functions are defined as the following: If <span class="math inline">\(SUM(X_1, \dots, X_n) \leq \kappa\)</span>, then <span class="math inline">\(GAP-TR_{\kappa, \tau} = 0\)</span> and If <span class="math inline">\(SUM(X_1, \dots, X_n) \geq \kappa + \tau\)</span>, then <span class="math inline">\(GAP-TR_{\kappa, \tau} = 1\)</span> . The function is undefined when <span class="math inline">\(\kappa &lt; SUM(X_1, \dots, X_n) &lt; \kappa + \tau\)</span>.</p>
<p>To show that it is impossible to compute differentially private <span class="math inline">\(GAP-TR_{\kappa, \tau}\)</span> without making at least <span class="math inline">\(O(\sqrt{n})\)</span> error, the authors show that any private protocol for computing <span class="math inline">\(GAP-TR_{\kappa, \tau}\)</span> is unable to distinguish between an input of at least <span class="math inline">\(\sqrt{n}\)</span> 1’s and an input of all 0’s. This is saying if the protocols spit similar answers for both inputs thereby erring by nearly <span class="math inline">\(O(\sqrt{n})\)</span>. By argument 1, they then go onto claim that a private protocol for <span class="math inline">\(SUM\)</span> on the same input would make twice the error with high probability. Thus the entire game is to show that upto <span class="math inline">\(\sqrt{n}\)</span> number of 1’s the protocol has no idea how to distinguish between inputs – so it spits out garbage.</p>
<p>They define a distribution <span class="math inline">\(A\)</span> on inputs from <span class="math inline">\(\{0, 1\}^n\)</span> such that <span class="math inline">\(X \sim A\)</span> where <span class="math inline">\(X\)</span> is a <span class="math inline">\(n\)</span> dimensional binary vector <span class="math inline">\((x_1, \dots, x_n)\)</span> such that <span class="math inline">\(\mathbb{P}[X_i = 1] = \alpha\)</span>, where <span class="math inline">\(\alpha = \frac{1}{\epsilon\sqrt{n}}\)</span>. <strong>All the magic is encoded in the way we pick this <span class="math inline">\(\alpha\)</span></strong>.</p>
<p>By the chernoff bound <span class="math inline">\(\mathbb{P}[\sum_{i=1}^n X_i \leq (1 - \gamma)n\alpha] \leq e^{-\frac{\alpha n\gamma^2}{2}}\)</span> as <span class="math inline">\(n\alpha\)</span> is the expected value of the sum of <span class="math inline">\(Bernoulli(\alpha)\)</span>. Plugging in the value of <span class="math inline">\(\gamma=1/2\)</span> and <span class="math inline">\(\alpha\)</span>, we get</p>
<p><span class="math display">\[\mathbb{P}_{X \sim A}[\sum_{i=1}^n X_i \leq \frac{\sqrt{n}}{2\epsilon}] \leq e^{-\frac{\sqrt{n}}{8\epsilon}}\]</span></p>
<p>The above statement is saying that the likelihood of seeing less than <span class="math inline">\(O(\sqrt{n})\)</span> 1’s in a vector sampled from <span class="math inline">\(A\)</span> is extremely low, thereby the sum of this vector will be greater than <span class="math inline">\(O(\sqrt{n})\)</span> with large probability. <strong>[See notes on concentration inequality to see why they had to pick <span class="math inline">\(O(\sqrt{n})\)</span>. For anything that decays quicker, concentration inequalities will go out of the window.]</strong></p>
<p>Define <span class="math inline">\(\tau = 0.5n\alpha\)</span>. Fix any <span class="math inline">\(\epsilon\)</span>-differentially private, local protocol <span class="math inline">\(\prod\)</span> for computing <span class="math inline">\(GAP-TR_{\kappa, \tau}\)</span> with <span class="math inline">\(\kappa=0\)</span>. Remember, that in the local model the curator/analyser is assumed to be deterministic. The curator, sees sanitised inputs <span class="math inline">\(c = (S(X_1), \dots, S(X_n))\)</span> which is the overall view of the execution of the protocol. It then applies a deterministic algorithm <span class="math inline">\(G\)</span> to <span class="math inline">\(c\)</span>, where <span class="math inline">\(G(c) \in \{ 0, 1\}\)</span> is the output of the protocol.</p>
<p>Let <span class="math inline">\(D\)</span> denote the set of input vectors for which the curator answers 1, i.e., <span class="math inline">\(D := \{c=\Big(S(X_1), \dots, S(X_n)\Big) : G(c) = 1\}\)</span>. There are <span class="math inline">\(2^n\)</span> possible input values to the protocol, let <span class="math inline">\(\mathbb{P}[D]\)</span> be the fraction of times <span class="math inline">\(\prod\)</span> answers 1. Fix <span class="math inline">\(p \in [0,1]\)</span>. We look at all possible algorithms by considering the probability of answering 1. The reason the authors can do this is because the curator is deterministic. It can only see a fixed number of views, after that it just says 1 or 0. So all algorithms can be divided into 2 groups: Ones that predict 1 less than <span class="math inline">\(p\)</span> fraction of the time and ones that do not. For each of these cases we show, that all protocols fail to distinguish between <span class="math inline">\(A\)</span> and <span class="math inline">\(0\)</span>.</p>
<p><strong>Case I:</strong> <span class="math inline">\(\mathbb{P}[D] &lt; p\)</span>. Let E denote the event of the <span class="math inline">\(\prod\)</span> making the following mistake : <span class="math inline">\(\sum_{i=1}^nX_i \geq O(\sqrt{n})\)</span> and the protocol returns 0. The complement <span class="math inline">\(E^c\)</span> of this event is <span class="math inline">\(\sum_{i=1}^nX_i \leq O(\sqrt{n})\)</span> or the protocol returns 1. Formally</p>
<span class="math display">\[\begin{align*}
\mathbb{P}_{X \sim A}[E^c] &amp;\leq \mathbb{P}[D] + \mathbb{P}[\sum_{i=1}^nX_i \leq O(\sqrt{n})] \\
&amp;\leq p + 0
\end{align*}\]</span>
<p>By the chernoff bound we have already shown that <span class="math inline">\(\mathbb{P}[\sum_{i=1}^nX_i \leq O(\sqrt{n})]\)</span> is so unlikely, it can be ignored as it is almost 0. The <span class="math inline">\(\mathbb{P}_{X \sim A}[E] \geq 1 - p\)</span></p>
<p><strong>What we have shown is that, all algorithm returns 0 most of the times [(1 - p) fraction of the time], if we sent the algorithm inputs from <span class="math inline">\(A\)</span> it would make mistakes with constant probability.</strong> NOTE: The reason we really needed <span class="math inline">\(O(\sqrt{n})\)</span> was to kill the Chernoff bound to 0. A sharper error rate would not have killed it off, and therefore we would not get the constant probability bound we get.</p>
<p>What about algorithms that mostly say 1. We now show those algorithms will also make an error with constant probability because they output zero vector inputs as 1 with constant probability. To be able to show this, we need to use some lemmas which have to do with the privacy requirements. <strong>NOTE: </strong> so far we have only used Chernoff-Hoeffding bounds – nothing about the privacy of algorithms have been utilised.** The proof for the lemmas is provided below the main proof.</p>
<p>Case II: <span class="math inline">\(\mathbb{P}[D] \geq p\)</span></p>
</div>
<h2 id="shuffle-privacy">Shuffle Privacy</h2>
<p>The following screenshot is taken from the <a href="https://www.youtube.com/watch?v=wkF_uBo-bLo">official talk</a> by one of the authors and provides an overview of shuffle privacy.</p>
<p><img src="../ShuffleSumBinaryRasmus/pngs/overview.png" height="300px" width="600px"></img></p>
<h4 id="borja-2020--private-summation-in-the-multi-message-shuffle-model">Borja 2020- Private Summation in the Multi-Message Shuffle Model</h4>
<p>An interesting feature of the shuffle model is that increasing the amount of messages sent by each user can lead to protocols with accuracies comparable to the ones achievable in the central model. In particular, for the problem of privately computing the sum of n bounded real values held by n different users</p>
<h2 id="references">References</h2>
<ol type="1">
<li><a href="../DiscreteLaplace/paper.pdf">UNIVERSALLY UTILITY-MAXIMIZING PRIVACY MECHANISMS</a></li>
</ol>
<ol start="2" type="1">
<li><a href="https://arxiv.org/pdf/1903.02837.pdf">The privacy blanket of the shuffle model</a></li>
</ol>
<ol start="3" type="1">
<li><a href="https://arxiv.org/pdf/1103.2626.pdf">Distributed Private Data Analysis: On Simultaneously Solving How and What</a>
</div></li>
</ol>
<div id="footer">
  “Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”― Richard Feynmann
</div>
</body>
</html>
