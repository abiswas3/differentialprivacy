<div class="container">

# NIPS 2021


* [Littlestone Classes are Privately Online Learnable](https://nips.cc/Conferences/2021/Schedule?showEvent=28079)
* [Covariance Aware Mean Estimation](https://nips.cc/Conferences/2021/Schedule?showEvent=26983)
* [Differentially Private Model Personalization](https://nips.cc/Conferences/2021/Schedule?showEvent=27821)
* Differentially Private Multi-Armed Bandits in the Shuffle Model	
* [Instance optimal mean estimation under DP](https://openreview.net/forum?id=AjgFqUoD4U)
* [Exact Privacy Guarantees for Markov Chain Implementation of Exp Mech](https://openreview.net/forum?id=SbGpYmQHlS8)
* Private and Non private uniform testing for ranking data
* [Differentially Private Sampling from Distributions](https://openreview.net/forum?id=6PoupJO89MG)
* 
* [Renyi Differential Privacy of The Subsampled Shuffle Model In Distributed Learning](https://openreview.net/forum?id=SPrVNsXnGd) 
* 
* [On the Sample Complexity of Privately Learning Axis-Aligned Rectangles](https://openreview.net/forum?id=Kzuys6WghCV)
* [Privately Learning Subspaces](https://openreview.net/forum?id=YBanVDVEbVe)
* [Individual Privacy Accounting via a RÃ©nyi Filter](https://openreview.net/forum?id=PBctz6_47ug)
* [Differentially Private Learning with Adaptive Clipping](https://openreview.net/forum?id=RUQ1zwZR8_)
* [Deep Learning with Label Differential Privacy](https://arxiv.org/pdf/2102.06062.pdf)

## [Robust and differentially private mean estimation](https://openreview.net/forum?id=CuQoImkKkIj)

Authors: Xiyang Liu, Weihao Kong, Sham M. Kakade, Sewoong Oh

### Problem 

In this paper,focus on one of the most canonical problems in statistics: estimating the mean of a distribution from i.i.d. samples. <


### Contribution

Private mean estimation for i.i.d samples has already been studied by Kamath and Vadhan. However, their approaches are claimed to be brittle when a fraction of the data is corrupted, posing a real threat, referred to as data poisoning attacks

In defense of such attacks, robust (but not necessarily private) statistics has emerged as a popular setting of recent algorithmic and mathematical breakthroughs.

So this is the first work to bring them together -- for sub-gaussian and heavy tailed noise distributions in high dimensions.

## [The Skellam Mechanism for Differentially Private Federated Learning](https://openreview.net/forum?id=dvyUaK4neD0)

### Problem

An alternative to the discreet gaussian due to nicer properties. I do not quite understand these nice properties yet.


## [Learning with User-Level Privacy](https://openreview.net/forum?id=G1jmxFOtY_)

### Problem


</div>	