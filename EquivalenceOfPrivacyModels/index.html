<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Readme</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../blog.css">
  <script src="../code/es5/tex-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/code/bootstrap.min.css">
  <script src="/code/jquery.min.js"></script>
  <script src="/code/bootstrap.min.js"></script>
  
  
  <ul class="bar">
    <li class="barli"><a class="active" href="/">Home</a></li>
    <li class="barli"><a href="/differentialprivacy/index.html">Notes on Differential Privacy</a></li>  
    <!-- <li class="barli"><a href="https://abiswas3.github.io/differentialprivacy.github.io">Notes on Differential Privacy</a></li> -->
  </ul>
</head>
<body>

<div class="container">
<p>Draft (first update): Novmber 2nd, 2021, 10:04 BST</p>
<p>Draft (last update): Novmber 2nd, 2021. 19:17 BST</p>
<h1 id="introduction">Introduction</h1>
Consider the problem of releasing the histogram sum
<div class="question">
(I think they are called counting queries or something else or succint histograms)
</div>
<p>of a population of integers privately under the three models:</p>
<ol type="1">
<li>Laplace Mechanism for Central Privacy</li>
<li>Binomial noise in Shuffle Privacy by <a href="ShufflePrivacy/index.html">Cheu</a></li>
<li>Sample and Threshold Method that was re-derived by Graham.</li>
<li><strong>TODO: <a href="ShuffleSumBinaryRasmus/">Ghazi, Rasmus Pagh – binary sums and histograms</a></strong> is also very closely connected. Working on this as next step if these proofs are correct.</li>
</ol>
We can show they 2 and 3 are equivalent. In terms of how 2 and 3 relate to 1:
<div class="intuition">
I have not proven this fact explicitly yet but I think I can show (1) just happens to be the “sharpest” sub-gaussian noise distribution we can sample from with tails fat enough for the right signal to noise. Conceptually I think it will end up looking like hinge loss and logistic loss where both work in practice. Logistic loss is smoother and nicer but the hinge loss is the tighest convex relaxation to the 0-1 loss.
</div>
<p>We first look at the binary summation problem and then for summation of integers.</p>
<h2 id="binary-sums">Binary Sums</h2>
<p>Each member of the dataset <span class="math inline">\(x \in D\)</span> holds a value in <span class="math inline">\(\{0, 1\}\)</span>. We simply want to estimate <span class="math inline">\(f(X) = \sum_{i=1}^n x_i\)</span>. Let <span class="math inline">\(\hat{f}(X)\)</span> be the estimated sum by the algorithms. Understanding binary sum gives us all the key insights we need to anslyse integer sums. In this writeup we consider the view of the anlyser for the three regimes in consideration. <strong>NOTE:</strong> this is purely for the sake of analysis. In practice they are usually implemented in a federated setting or local/shuffle privacy setting. The noise addition, thresholding and sampling might be done at the user level. We perform all the computations at the analyser level as it simplifies the analysis.</p>
<p>Under the <a href="Definitions/">central privacy</a>, the anlayser sees all the inputs, sums them and adds one instance noise drawn from a Laplace distribution. See picture below:</p>
<div class="algorithm">
<p><img src=pngs/central.png></img></p>
<p><span class="math inline">\(\hat{f(X)} = \sum_{i=1}^n x_i + Y\)</span></p>
<p>where <span class="math inline">\(Y \sim Lap(\frac{1}{\epsilon})\)</span></p>
</div>
<p>In shuffle privacy, under the <a href="ShufflePrivacy/index.html">“The binary histogram”</a> section of this writeup we show that the authors just draw <span class="math inline">\(n\)</span> random variables <span class="math inline">\(z_i \sim Bernoulli(p_1)\)</span> and add them <span class="math inline">\(f(X)\)</span>. Let <span class="math inline">\(Y= \sum_{i=1}^n z_i\)</span>, the final answer is just the expected value of <span class="math inline">\(Y\)</span> subtracted from <span class="math inline">\(f(X)\)</span>. In all these shuffle privacy papers which claim near central accuracy, this is repeated theme. <span class="math inline">\(n\)</span> units of little noise that when added up – pretend to be one unit from a smooth sub gaussian distribution. In <a href="ShuffleSumBinaryRasmus/">Ghazi, Rasmus Pagh – binary sums and histograms</a> they dive deep into this with Poisson, Negative Binomial and Discrete Laplace distributions (all infinitely divisible distributions) See picture below:</p>
<div class="algorithm">
<p><img src=pngs/shuffle.png></img></p>
<p><span class="math inline">\(\hat{f}(X) = \sum_{i=1}^n (x_i + z_i) - E[\sum_{i=1}^n z_i]\)</span></p>
<p><span class="math inline">\(\hat{f}(X) = \sum_{i=1}^n (x_i + z_i) - np_1\)</span></p>
<p>where <span class="math inline">\(z_i \sim Bernoulli(p_1))\)</span></p>
</div>
<p>Both methods add one single instance of noise to the truth: (1) Adds one Laplacian random variable (2) Adds one random variable drawn from a <span class="math inline">\(Binomial(n, p_1)\)</span>. All the work in <a href="ShufflePrivacy/index.html">Cheu</a> is to figure out what values of <span class="math inline">\(p\)</span> is acceptable given <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span> to ensure privacy. If one were to plot the formulae they use, it shows that only a restricted range of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span> is possible.</p>
<p>Now consider a simplified version of the sample and threshold algorithm. For simplicity assume <span class="math inline">\(\tau = 0\)</span> i.e. there is no thresholding. We’ll analyse the impact of thresholding in later section <strong>Should not be too hard to show it only helps</strong>.</p>
<p>The analyser receives values from <span class="math inline">\(n\)</span> users. For each user, it generates a random variable <span class="math inline">\(z_i \sim Bernoulli(p_2)\)</span>. If the <span class="math inline">\(z_i\)</span> is heads it includes sample <span class="math inline">\(x_i\)</span> in the calculation for the sum, otherwise it does not. This describes Poisson sampling as described in the paper. Thus algorithm is outputting <span class="math inline">\(\hat{f}(X) = \sum_{i=1}^n z_i x_i\)</span>. We now show that with a bit of simple algebra, this algorithm is equivalent to the shuffle privacy algorithm for the case of Binary sums.</p>
<p>Define the bit flipping operator as <span class="math inline">\(\bar{x} = 1 - x\)</span>. For each <span class="math inline">\(z_i x_i\)</span> the analyser sees, assume it just flips the output to <span class="math inline">\(\bar{x_i z_i}\)</span> and uses this sum instead. We are interested in an estimator that in expectation gives us <span class="math inline">\(\sum_{i=1}^n x_i\)</span>. Note: there are exactly the same number of random variables in this method as there is in the shuffle method.</p>
<div class="algorithm">
<p><img src=pngs/sample.png></img></p>
<span class="math display">\[\begin{align*}
c &amp;= \sum_{i=1}^n\bar{x_i z_i} \\
&amp;= \sum_{i=1}^n(\bar{x_i} + \bar{z_i} )\tag{1}\label{1} \\
&amp;= (n - \sum_{i=1}^n x_i) + (n - \sum_{i=1}^n z_i) \\
&amp;= 2n - \sum_{i=1}^n x_i - \sum_{i=1}^n z_i
\end{align*}\]</span>
<p><span class="math inline">\(\ref{1}:\)</span> simple boolean algebra, De-Morgan’s Law</p>
<p>We can just normalise this by outputting, <span class="math inline">\(\hat{f(X)} = -c + 2n - np_2\)</span> which in expectation gives us what we want. Also note that that variance of both estimators are the exact same. In fact they are practically equivalent.</p>
</div>
<h2 id="connection-between-sampling-and-shuffle-privacy-for-histogram-sums">Connection between Sampling and Shuffle privacy for histogram sums</h2>
<p>We have shown that for binary sums, shuffle privacy and sample privacy are equivalent. We now look at the shuffle privacy algorithm <a href="ShufflePrivacy/index.html">described in detail here</a>.</p>
<p>Now each user has a value <span class="math inline">\(x \in \{1, 2, ..., d \}\)</span>. As shorthand we right <span class="math inline">\(x_i \in \left\lfloor d \right\rfloor\)</span>. Cheu et al, just decompose this into <span class="math inline">\(d\)</span> binary sum problems for each <span class="math inline">\(d\)</span>. For a particular <span class="math inline">\(j \in \left\lfloor d \right\rfloor\)</span>, all <span class="math inline">\(x_i=j\)</span> are set to 1 and the rest are set to 0. Then the analyser outputs <span class="math inline">\(j \times \hat{f}(X)\)</span> by running the previous algorithm.</p>
<p>Now consider the sampling algorithm. We do the exact same pre-processing for each <span class="math inline">\(j \in \left\lfloor d \right\rfloor\)</span>. Now instead of additive bernoulli noise we just sample/multiply with the random variables as shown above. We have shown equivalence.</p>
<h1 id="references">References</h1>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2109.13158.pdf">Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message</a></li>
</ol>
</div>
<div id="footer">
  “Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”― Richard Feynmann
</div>
</body>
</html>
