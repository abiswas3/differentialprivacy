[["index.html", "My Notes 1 My notes", " My Notes Ari 2022-06-29 1 My notes This page is a repository of notes I’ve written to teach myself some mathematics "],["intro.html", " 2 Notes on DP", " 2 Notes on DP #differential_privacy You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2022) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) ad (Ghazi et al. 2019) References "],["smooth-distributions.html", " 3 Smooth Distributions", " 3 Smooth Distributions #differential_privacy We are interested in releasing counting queries \\(q\\) that are \\(k\\)-incremental Definition 3.1 (K-incremental functions) (Ghazi et al. 2019) A function \\(q: \\mathcal{X}^n \\rightarrow \\mathbb{Z}^M\\) is said to be \\(k\\)-incremental if for all neighbouring datasets \\(X \\sim X^{\\prime}\\), \\(|| f(X) - f(X^{\\prime})||_{\\infty} \\leq k\\). Theorem 3.1 (Smooth Distributions guarantee DP) Suppose \\(q: \\mathcal{X}^n \\rightarrow \\mathbb{Z}^M\\) is \\(k\\)-incremental i.e. for all neighbouring datasets \\(X \\sim X^{\\prime}\\) we have \\(||q(X) - q(X^{\\prime}) ||_{\\infty} = k\\) and \\(\\Delta(q) = || q(X) - q(X^{\\prime}) ||_1 = \\Delta\\). Let \\(\\mathcal{D}\\) be a \\((\\varepsilon, \\delta, k)\\)-smooth distribution. Then the mechanism \\(M\\) \\[\\begin{equation*} M_{(Y_1, \\dots, Y_M)}(X, q) = q(X) + (Y_1, \\dots, Y_M) \\end{equation*}\\] is \\((\\epsilon\\Delta, \\delta\\Delta)\\) differentially private, where \\((Y_1, \\dots, Y_M) \\overset{\\text{i.i.d}}{\\sim} D.\\) Proof. Let \\(X = (x_1, \\dots, x_n)\\) where \\(x_i \\in \\mathcal{X}\\) and \\(X^\\prime = (x_1, \\dots, x_n^\\prime)\\). Let \\(\\vec{y} = (y_1, \\dots, y_M)\\) and \\(\\vec{Y} = (Y_1, \\dots, Y_M)\\) be i.i.d draws from \\(\\mathcal{D}\\). Assume that Equation (3.1) holds \\[\\begin{equation} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{[g(\\vec{y}) \\geq e^{\\varepsilon^\\prime}]} \\leq \\delta^\\prime \\tag{3.1} \\end{equation}\\] where \\(g(\\vec{y}) = \\frac{\\Pr_{(\\vec{Y} \\sim \\mathcal{D})}{[M_{\\vec{Y}}(X, q) = q(X) + \\vec{y} ]}}{\\Pr_{(\\vec{Y} \\sim \\mathcal{D})}{[M_{\\vec{Y}}(X^\\prime, q) = q(X^\\prime) + \\vec{y} ]}}\\). Let \\(S \\subseteq \\mathbb{Z}^M\\) be an arbitrary subset in the range of \\(M\\). Let \\(T = \\{M_{\\vec{y}(X, q)} | g(\\vec{y}) &lt; e^{\\varepsilon^\\prime}\\}\\) represent a set of outputs of \\(M\\) over draws of \\(\\vec{y}\\) such that \\(g(\\vec{y}) &lt; e^{\\varepsilon^\\prime}\\). Then from equation (3.1) we can show that \\(M\\) is \\((\\varepsilon^\\prime, \\delta^\\prime)\\) differentially private \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X, q) \\in S]} &amp;\\leq \\delta^\\prime + \\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X, q) \\in S \\cap T]} \\tag{3.2}\\\\ &amp;= \\delta^\\prime + \\sum_{w \\in S \\cap T }\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X, q) = w]} \\\\ &amp;\\leq \\delta^\\prime + \\sum_{w \\in S \\cap T }e^{\\varepsilon^\\prime}\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X^\\prime, q) = w]} \\tag{3.3}\\\\ &amp;\\leq \\delta^\\prime + \\sum_{w \\in S}e^{\\varepsilon^\\prime}\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X^\\prime, q) = w]} \\tag{3.4}\\\\ &amp;= \\delta^\\prime + e^{\\varepsilon^\\prime}\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X^\\prime, q) \\in S]} \\end{align}\\] Equation (3.2) is from the law of total probability, equation (3.2) comes from equation (3.1) assumption and equation (3.4) is true as \\(T \\cap S \\subseteq S\\). Therefore all there is to do is to show that equation (3.1) is true if \\(\\mathcal{D}\\) is as defined and \\(\\varepsilon^\\prime = \\varepsilon\\Delta, \\delta^\\prime= \\delta\\Delta\\) to complete the proof. Define \\(k_j = q(X)_j - q(X^\\prime)_j\\). As each coordinate of \\(q(X)\\) is independently perturbed and \\(q\\) is a deterministic function, equation @ref(eq: smooth-dp) is equivalent to equation (3.5). \\[\\begin{equation} \\tag{3.5} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\prod_{j=1}^M \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{\\varepsilon^\\prime}\\Bigg]} \\leq \\delta^\\prime \\end{equation}\\] Thus, in order to prove equation (3.1) is true, it suffices to show that equation (3.5) holds. We know that \\(\\mathcal{D}\\) is a smooth distribution i.e for each \\(j \\in [M]\\) \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{|k_j|\\varepsilon}\\Bigg]} \\leq \\delta \\end{align}\\] We can apply the union bound to get the probability that the joint distribution over all indices to get \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\prod_{j=1}^M \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{\\sum_{j=1}^m|k_j|\\varepsilon}\\Bigg]} \\leq \\delta \\sum_{j=1}^M \\mathbb{I}(k_j \\neq 0) \\end{align}\\] Given the sensitivity of \\(q\\) is \\(\\Delta\\), at most \\(\\Delta\\) indices for \\(k_j\\) can be non zero and \\((\\sum_{j=1}^M |k_j|) = \\Delta\\). Finally we get the result we want \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{\\prod_{j=1}^M [Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{\\Delta\\varepsilon}\\Bigg]} \\leq \\delta \\Delta \\label{eq: non_zero} \\end{align}\\] Ahoy 3.1 Lemma 3.1 Let \\(n \\in \\mathbb{N}\\), \\(p \\in [0, 1/2]\\), \\(\\alpha \\in [0,1)\\) and \\(k \\leq \\frac{n\\alpha p}{2}\\). Then the binomial distribution \\(Bin(n, p)\\) is a \\((\\varepsilon, \\delta, k)\\)-smooth distribution. Proof. Let \\(Y \\sim Bin(np)\\), then \\(\\Pr{[Y = y]} = {n \\choose y}p^{y}(1-p)^{n-y}\\). For any \\(-k \\leq k^\\prime \\leq k\\), define an interval \\(\\mathcal{\\varepsilon} := [(1 - \\alpha)np + k^\\prime, (1 + \\alpha)np - k^\\prime]\\). This an interval of size \\(k\\) around the mean of the distribution. Note that as long as \\(k \\leq \\frac{np}{2}\\alpha\\), then the interval \\(\\mathcal{\\varepsilon^\\prime} := [(1 - \\alpha/2)np, (1 + \\alpha/2)np]\\) is contained inside of \\(\\mathcal{\\varepsilon}\\). Thus if \\(y \\sim Bin(np)\\) is not in \\(\\mathcal{\\varepsilon}\\), it is also not inside \\(\\mathcal{\\varepsilon^\\prime}\\). We know how to bound the probability that \\(y \\notin \\mathcal{\\varepsilon^\\prime}\\) by using the multiplicative chernoff bound. Invoking it, we get \\[\\begin{align} \\Pr_{y \\sim Bin(n,p)}{[y \\notin \\epsilon]} &amp;\\leq \\Pr_{y \\sim Bin(n,p)}{[y \\notin \\epsilon^\\prime]} \\\\ &amp;\\leq e^{-\\frac{-\\alpha^2np}{8}} + e^{-\\frac{-\\alpha^2np}{8+2\\alpha}} \\\\ &amp;= \\delta \\\\ \\end{align}\\] Now for all \\(y \\in \\mathcal{\\varepsilon}\\), we have for \\(0 \\leq k^\\prime \\leq k\\) \\[\\begin{align} \\frac{\\Pr{[Y = y]}}{\\Pr{[Y = y + k^\\prime]}} &amp;= \\Big(\\frac{1-p}{p}\\Big)^{k^\\prime} \\prod_{i=1}^{k^\\prime} \\frac{y + i}{n-y -i+1} \\\\ &amp;\\leq \\Big(\\frac{1-p}{p}\\Big)^{k^\\prime} \\Big(\\frac{y + k^\\prime}{n-y -k^\\prime}\\Big)^{k^\\prime} \\\\ &amp;\\leq \\Big(\\frac{1-p}{p}\\Big)^{k^\\prime} \\Big(\\frac{(1 + \\alpha)np}{n- (1 + \\alpha)np}\\Big)^{k^\\prime} \\tag{3.6}\\\\ &amp;= (1 + \\alpha)^{k^\\prime} \\Big(\\frac{1-p}{1- p - p\\alpha}\\Big)^{k^\\prime} \\\\ \\end{align}\\] TODO Ari/Graham fix the the last inequality (3.6) comes from our assumption \\(y \\in \\mathcal{\\varepsilon}\\) and so when \\(y=(1 + \\alpha)np - k^\\prime\\) the ratio above is the largest. Similarly for \\(-k \\leq k^\\prime \\leq 0\\) we have \\[\\begin{align} \\frac{\\Pr{[Y = y]}}{\\Pr{[Y = y + |k^\\prime |]}} &amp;= \\Big(\\frac{p}{1-p}\\Big)^{|k^\\prime|} \\prod_{i=1}^{|k^\\prime|} \\frac{n-y +i}{y - i + 1} \\\\ &amp;\\leq \\Big(\\frac{p}{1-p}\\Big)^{|k^\\prime|} \\Big( \\frac{n-y +|k^\\prime|}{y - |k^\\prime|}\\Big)^{|k^\\prime|} \\\\ &amp;\\leq \\Big( \\frac{1 + p\\alpha - p}{(1-\\alpha)(1 - p)}\\Big)^{|k^\\prime|} \\label{eq: plug_in} \\\\ &amp;\\leq \\Big(\\frac{1 + \\alpha}{1 - \\alpha}\\Big)^{|k^\\prime|}\\tag{3.7} \\\\ &amp;= e^{|k|\\varepsilon} \\end{align}\\] (3.6) comes from plugging in the smallest value for \\(y\\) and (3.7) comes from the fact that \\(p \\leq 1/2\\) Finally we can prove smoothness using bayes rule. Let \\(g(y) = \\frac{Pr_{Y \\sim Bin(n,p)}[Y = y]}{Pr_{Y \\sim Bin(n, p)}[Y = y+k^{\\prime}]}\\) \\[\\begin{align} Pr_{y \\sim Bin(n, p)} \\Big[ g(y) \\geq e^{|k^{\\prime}|\\epsilon}\\Big] &amp;\\leq Pr_{y \\sim Bin(n, p)} \\Big[ g(y) \\geq e^{|k^{\\prime}|\\epsilon} \\Big| y \\notin \\mathcal{\\varepsilon} \\Big] + \\delta \\\\ &amp;\\leq e^{-\\frac{-\\alpha^2np}{8}} + e^{-\\frac{-\\alpha^2np}{8+2\\alpha}} \\tag{3.8} \\\\ &amp;\\leq \\delta \\end{align}\\] (3.8) comes from the fact that when \\(y \\in \\mathcal{\\varepsilon}\\) we have \\(\\frac{\\Pr{[Y = y]}}{\\Pr{[Y = y + k^\\prime]}} \\leq e^{\\varepsilon}|k^\\prime|\\) and by how we defined \\(\\varepsilon\\) in equation (3.7). So we have \\(Pr_{y \\sim Bin(n, p)} \\Bigg[ \\frac{Pr_{Y \\sim Bin(n,p)}[Y = y]}{Pr_{Y \\sim Bin(n, p)}[Y = y+k^{\\prime}]} \\geq e^{|k^{\\prime}|\\epsilon} \\Big| y \\notin \\mathcal{\\varepsilon} \\Bigg] = 0\\) References "],["beavers-triples.html", " 4 Beavers Triples", " 4 Beavers Triples #cryptography #mpc Todo SNIPS notes "],["simulation-for-mpc.html", " 5 Simulation for MPC 5.1 Bit oblivious transfer 5.2 Static Semi Honest PPT Adversaries 5.3 Static Malicious PPT Adversaries 5.4 Covert OT", " 5 Simulation for MPC #cryptography #mpc oblivious transfer simulation Let’s say we have \\(n\\) parties indexed by \\(\\{1, \\dots, n \\}\\). Each party possesses private information \\(x_i \\in \\mathbb{Z}\\) that they do not want anyone else to know. However, each person also wishes to compute some function over everyone’s private values. For example, they might want to compute \\(f\\) where \\(f\\) is the sum of their values \\(f(x_1, \\dots, x_n) = \\sum_{i=1}^n x_i\\). One way to compute such an \\(f\\) is for everyone to send their private inputs to a trusted third party, which then outputs only the sum of the inputs to each party and nothing else. Caption In reality, finding a third party we can trust with our data is difficult (how on earth can we manifest Mufasa ?). So instead, we must design a protocol \\(\\pi_f\\) which specifies messages each party must send each other to compute their desired function \\(f\\). For this protocol \\(\\pi_f\\) to be helpful, it must satisfy some properties. Firstly, we must ensure that if every party followed the instructions by \\(\\pi_f\\), we would compute \\(f\\) correctly. If \\(f\\) is deterministic, then the output of \\(\\pi_f(x_1, \\dots, x_n\\) must equal \\(f(x_1, \\dots, x_n)\\). If \\(f\\) is random, the two protocols should be similar in distribution. Depending on the setting used, the definition of ‘similar’ changes, but let’s not bother with it now. Secondly, it must be secure. Intuitively we want the protocol to reflect our desire that no one else learns anything about our private values. However, learning anything is hard to define mathematically. We can try to exhaustively list everything we do not want an adversary \\(\\mathcal{A}\\) to learn, but this is not very practical. We do not know what information \\(\\mathcal{A}\\) will have when using the protocol. For example, the excerpt from (Lindell 2017) describes why the exhaustive listing is not a great idea. If we say that an adversary receiving a ciphertext cannot output any information about the plaintext, what happens if the adversary already has information about the plaintext? For example, the adversary may know that it is an English text. Of course, this has nothing to do with the security of the scheme since the adversary knew this beforehand and independently of the ciphertext So instead, we need a different approach to declare that \\(\\pi_f\\) is secure. We first define some limits on the adversarial behaviour we hope to secure against. Note that this definition differs from trying to list everything an adversary can do. It is more of a general description of restrictions we place on adversaries or the class of adversaries we want to defend against. For example, an adversary may be computationally unbounded or restricted to polynomial-time probabilistic computations. An adversary could be passive, i.e. they are forced to follow the instructions of a protocol and can only hope to learn side information by looking at the transcript of the protocol. The adversary could be active or malicious in that they arbitrarily deviate from the \\(\\pi_f\\)’s specification. An adversary could be static, i.e. their plan of attack is fixed before executing \\(\\pi_f\\) or dynamic in which they adapt their strategy based on previous messages. Once we define the class of adversaries we want to defend against; we return to our perfect world with Mufasa. Then we construct an algorithm (referred to as the simulator) that interacts with the ideal functionality and interacts with the adversary \\(\\mathcal{A}\\) and outputs some information. Informally a protocol \\(\\pi_f\\) is secure if \\(\\mathcal{A}\\) can do as much harm in the real world as they are in this perfect world with Mufasa. Put another way; we would like to show that there is a simulator that can output any information that \\(\\mathcal{A}\\) can learn in the real world. If this is the case, then \\(\\mathcal{A}\\) did not do any real harm as whatever they were able to learn could be learned in our perfect protected world. This discussion is quite abstract, and it might be hard to understand how to construct a secure protocol in practice. To better understand the security of MPC protocols, we look at three protocols that help us compute the oblivious transfer function with the most common classes of adversaries. We choose oblivious transfer, as it is a fundamental MPC primitive that enables us to compute general arithmetic and boolean circuits securely. 5.1 Bit oblivious transfer We consider the bit oblivious transfer functionality, defined by \\(f_{OT}\\Big((b0,b1),\\sigma \\Big) = (\\lambda,b_\\sigma)\\), where \\(b_0,b_1 ,\\sigma ∈ \\{0,1\\}\\). In words, \\(P_1\\) has a pair of input bits \\((b_0,b_1)\\) and P2 has a choice bit \\(\\sigma\\). The function is such that P1 receives no output (denoted by the empty string \\(\\lambda\\)), and in particular learns nothing about \\(\\sigma\\). In contrast, P2 receives the bit of its choice \\(b_\\sigma\\) and learns nothing about the other bit \\(b_{1−\\sigma}\\). SEE PICTURE In this writeup, we only consider static adversaries that are computationally bounded, i.e., PPT. Additionally, as OT is a two-party protocol, we assume that the adversary \\(\\mathcal{A}\\) controls one party and is responsible for sending messages. 5.2 Static Semi Honest PPT Adversaries These are the weakest class of adversaries we work with. Such adversaries cannot force a party to deviate from \\(\\pi_f\\). They can only do bad things with what they see while executing \\(\\pi_f\\). To define security we first have to describe some general notation. We write down the definition in terms of a 2 party protocol but it can be extended for \\(n\\) party protocol.s Let \\(f = (f1,f2)\\) be a probabilistic polynomial-time functionality and let \\(\\pi\\) be a two-party protocol for computing \\(f\\). The view of the \\(i\\)’th party \\(i \\in \\{1, 2\\}\\) during the execution of \\(\\pi\\) on inputs \\((x,y)\\) and security parameter \\(n\\) is denoted by \\(view_i^\\pi(x,y,n)=(w; r^i, m_1^i, \\dots, m_t^i)\\) where \\(w \\in \\{ x, y\\}\\) depending on the input value of the \\(i\\)’th party, \\(r^i\\) is the contents of the internal random tape of the \\(i\\)’th party and \\(m_j^i\\) represents the \\(j\\)’th message it received from the other party. The output of the \\(i\\)’th party denoted by \\(output_i^\\pi(x,y,n)\\) is a PPT function of the view of the \\(i\\)’th party. The final output of the protcol is given by \\(output^\\pi\\big(1^n, x, y, n \\big) = \\Big(output_1^\\pi\\big(1^n, x, y, n \\big), output_2^\\pi\\big(1^n, x, y, n \\big)\\Big)\\) In the real world, the two parties execute the protocol by exchanging messages with each \\(\\alpha\\) other. Caption Definition 5.1 (Security against static semi-honest adversaries) We say that \\(\\pi\\) securely computes \\(f\\) in presence of static semi-honest adversaries if there exists PPT algorithms \\(S_1\\) and \\(S_2\\) such that the joint distribution of the following probability ensembles are computationally indistinguishable \\[\\begin{align} \\{ \\Big(S_1 \\big(1^n, x, f_1(x,y) \\big), f(x,y)\\Big) \\}_{x, y, n} \\overset{\\mathsf{comp}}\\equiv \\{ \\Big(view_1^\\pi\\Big(1^n, x, y, n \\Big), output^\\pi\\big(1^n, x, y, n \\big)\\Big)\\}_{x, y, n} \\end{align}\\] \\[\\begin{align} \\{ \\Big(S_2 \\big(1^n, x, f_1(x,y) \\big), f(x,y)\\Big) \\}_{x, y, n} \\overset{\\mathsf{comp}}\\equiv \\{ \\Big(view_2^\\pi\\Big(1^n, x, y, n \\Big), output^\\pi\\big(1^n, x, y, n \\big)\\Big)\\}_{x, y, n} \\end{align}\\] where \\(x,y \\in \\{ 0, 1\\}^*\\) such that \\(|x| = |y|\\) and \\(n \\in \\mathbb{N}\\) 5.2.1 Background Assume we have a set of trapdoor permutations1 \\(\\{ f_1, f_2, \\dots, f_n\\}\\)with the property that if we randomly sampled a permutation function from this set, it would be hard to invert. \\(I(1^n)\\) is a PPT algorithm that selects a random \\(n\\)-bit index \\(\\alpha\\) to obtain \\(f_{\\alpha}\\) along with a corresponding trapdoor \\(\\tau\\). \\(S(\\alpha)\\) is a PPT algorithm that uniformly samples an element from the domain for \\(f_\\alpha\\) (or equivalently the range as \\(f_\\alpha\\) is a permutation) \\(F(\\alpha, x)\\) is a PPT algorithm to compute \\(f_{\\alpha}(x)\\) \\(F^{-1}(\\tau, y)\\) is a PPT algorithm to compute \\(f_{\\alpha}^{-1}(y)\\) Next, we describe \\(\\pi_{OT}\\) for computing \\(f_{OT}\\) and show that it is secure in the presence of semi-honest adversaries. 5.2.2 Protocol Semi Honest OT It is quite easy to see that the protocol is correct and \\(P_2\\) does receive \\(b_\\sigma\\). It is less obvious that it is secure. Hence, we have to construct a simulator which is able to output the same view as the real world adversary controlling party \\(P_1\\) or \\(P_2\\). Proof. Assume that \\(P_1\\) has been corrupted. Note that \\(P_1\\) has no output, thus all the simulator has to do is to output the \\(P_1\\)’s view i.e. it has to simulate the messages \\(y_0, y_1\\) it receives from \\(P_2\\). We construct the following simulator \\(S_1\\): \\(S_1\\) already has \\((b_0, b_1)\\) as input \\(S_1\\) chooses a uniformly distributed random tape \\(r\\) for \\(P_1\\) (of the length required, which is what is needed to run \\(I\\)). \\(S_1\\) runs \\(I(1^n)\\) such that \\((\\alpha, \\tau) \\xleftarrow R I(1^n)\\). It then selects \\(y_0, y_1\\) uniformly randomly from the domain of \\(f_{\\alpha}\\) by running \\(S(\\alpha)\\) twice. Finally \\(S_1\\) outputs \\((b_0, b_1), r, (y_0, y_1)\\). We have constructed a simulator in polynomial time, and now we have to show it’s output is indistinguishable from the output of the real-world adversary. The definition of a collection of trapdoor permutation states that \\(S(\\alpha)\\) outputs a value that is almost uniformly distributed in the domain of \\(f_\\alpha\\) (and the domain equals the range, since it is a permutation). Thus we have that the distribution \\(S(\\alpha)\\) is statistically close to the distribution \\(F(\\alpha, S(\\alpha)\\). Therefore we can conclude that \\[\\begin{align} \\{ (y_0, F(\\alpha, x_1)\\} &amp;\\overset{\\mathsf{comp}}\\equiv \\{ (y_0, y_1\\} &amp;\\overset{\\mathsf{comp}}\\equiv \\{ (F(\\alpha, x_0), y_1\\} \\end{align}\\] Thus regardless of what value of \\(\\sigma\\) \\(P_2\\) picks, the simulator can generate a report that is indistinguishable from that of the real world adversary. Now consider when \\(P_2\\) has been corrupted. \\(P_2\\)’s view includes It’s random tape \\(r\\) It’s input \\(\\sigma \\in \\{ 0, 1\\}\\) The messages \\(\\alpha, \\beta_0, \\beta_1\\) Note that \\(S_2\\) receives \\(b_\\sigma\\) from the trusted third party. So it can set \\(\\beta_\\sigma = B(x_\\sigma, \\alpha) \\oplus b_\\sigma\\) where \\((\\alpha, \\tau) \\xleftarrow R I(1^n)\\) and \\(x_\\sigma \\xleftarrow S(\\alpha)\\). The challenge is to simulate \\(\\beta_{1 - \\sigma}\\) as \\(S_2\\) does not know what \\(b_{1 - \\sigma}\\) is. Keeping this in mind we construct the following simulator. \\(S_2\\) picks two random tapes \\(r_0\\) and \\(r_1\\)2. \\(S_2\\) runs \\(I(1^n)\\) such that \\((\\alpha, \\tau) \\xleftarrow R I(1^n)\\) \\(S_2\\) computes \\(x_\\sigma \\xleftarrow{r_\\sigma} S(\\alpha\\) where \\(\\xleftarrow{r_\\sigma}\\) denotes sampling using random tape \\(r_\\sigma\\). It then samples \\(y_{1 - \\sigma} \\xleftarrow{r_{1 - \\sigma}} S(\\alpha)\\) and computes \\(x_{1 - \\sigma} = F^{-1}(\\tau, y_{1 - \\sigma})\\) \\(S_2\\) computes \\(\\beta_{\\sigma} = B(\\alpha, x_\\sigma) \\oplus b_\\sigma\\) and \\(\\beta_{1 -\\sigma} = B(\\alpha, x_{1 - \\sigma})\\) It outputs \\((\\sigma, b_\\sigma, r_1, r_2, \\beta_0, \\beta_1)\\) as the output to the distinguisher. Comparing the view of the simulator (RHS) and the real world adversary (LHS) we get \\[\\begin{align} \\text{REAL WORLD } &amp;| \\text{ SIMULATOR} \\\\ \\Big(\\sigma, b_\\sigma, r_1, r_2, B(\\alpha, x_\\sigma) \\oplus b_\\sigma, B(\\alpha, x_{1 - \\sigma}) \\oplus b_{1 - \\sigma}\\Big) &amp;\\Bigg| \\Big(\\sigma, b_\\sigma, r_1, r_2, B(\\alpha, x_\\sigma) \\oplus b_\\sigma, B(\\alpha, x_{1 - \\sigma})\\Big) \\\\ \\Big(\\dots, B(\\alpha, x_{1 - \\sigma}) \\oplus b_{1 - \\sigma}\\Big) &amp;\\Bigg| \\Big(\\dots, B(\\alpha, x_{1 - \\sigma})\\Big) \\end{align}\\] Note the only difference between them is that for the simulator we have \\(\\beta_{1 - \\sigma} = B(\\alpha, x_{1 - \\sigma})\\) whereas for the real world adversary it is \\(B(\\alpha, x_{1 - sigma}) \\oplus b_{1 - \\sigma}\\). If \\(b_{1 - \\sigma}=0\\), then two values are the same and we are done. So all we have to show is that when \\(b_{1 - \\sigma}=1\\), we have for all \\(\\sigma, b_{\\sigma} \\in \\{ 0, 1\\}\\) we have that \\(B(\\alpha, x_{1 - \\sigma}) \\equiv B(\\alpha, x_{1 - sigma}) \\oplus 1\\) Assume that the above statement is false i.e. there exists a PPT distinguisher \\(D\\) such that for infinitely many values of the triple \\((\\sigma, b_{\\sigma}, n)\\) we have for some polynomial \\(p\\) \\[\\begin{align} \\mathcal{D}(\\beta_0, \\beta_1) \\\\ =Pr{[D\\Big(\\dots, B(\\alpha, x_{1 - \\sigma})\\Big) = 1]} - Pr{[D\\Big(\\dots, B(\\alpha, x_{1 - \\sigma}) \\oplus 1 \\Big) = 1]} \\geq \\frac{1}{p(n)} \\tag{5.1} \\end{align}\\] Without loss of generality, we assume that for infinitely many \\(n\\)’s, \\(D\\) outputs 1 with greater or equal probability when receiving \\(B(\\alpha, x_{1−\\sigma})\\) than when receiving \\(B(\\alpha, x_{1−\\sigma}) \\oplus 1\\). We will construct a PPT algorithm \\(\\mathcal{A}\\) that is a simple wrapper around \\(D\\) to guess the bit of hardcore predicate function \\(B\\), leading to a contradiction – which will imply the above assumption is false. \\(\\mathcal{A}\\) has input \\(\\alpha, \\sigma, b_\\sigma, r_1, r_2\\) and sets \\(x_{1 - \\sigma} = F^{-1}(\\tau, y_{1- \\sigma})\\) where \\(y_{1- \\sigma} \\xleftarrow{r_{1 - \\sigma}} S(\\alpha)\\). It also computes \\(x_\\sigma \\xleftarrow{r_\\sigma} S(\\alpha)\\) and \\(\\beta_\\sigma = B(\\alpha, x_\\sigma) \\oplus b_\\sigma\\). It sets \\(\\beta_{1 - \\sigma}^{guess} \\xleftarrow{R} \\{ 0, 1\\}\\) to a random value. It then invokes \\(D\\) with the input \\(\\Big(\\sigma, b_\\sigma, r_{\\sigma}, r_{1 - \\sigma}, \\beta_\\sigma, \\beta_{1 - \\sigma}^{guess}\\Big)\\). If \\(D\\) outputs 1, then \\(\\mathcal{A}\\) leaves \\(\\beta_{1 - \\sigma}^{guess}\\) unchanged, otherwise it flips the bit i.e \\(\\beta_{1 - \\sigma}^{guess} = \\beta_{1 - \\sigma}^{guess} \\oplus 1\\) Remember this entire analysis is for \\(b_{1 - \\sigma} = 1\\); and in the simulator world \\(\\beta_{1 - \\sigma} = B(\\alpha, x_{1 - \\sigma})\\). Thus \\(\\mathcal{A}\\) guesses correctly i.e. \\(\\beta_{1 - \\sigma}^{guess} = \\beta_{1 - \\sigma} = B(\\alpha, x_{1 - \\sigma})\\), then it invokes \\(D\\) on input \\((\\dots, B(\\alpha, x_{1 - \\sigma}))\\). Otherwise \\(D\\) is invoked on input \\((\\dots, 1 \\oplus B(\\alpha, x_{1 - \\sigma}))\\). Setting \\(x = x_{1 - \\sigma}\\) and \\(\\beta = \\beta_{1 - \\sigma}^{guess}\\) to make notation easier, we have \\[\\begin{align} Pr{[\\mathcal{A}\\text{ computes } B(\\alpha, x)]} &amp;= \\frac{1}{2}Pr{[\\mathcal{A}\\text{ computes } B(\\alpha, x)| \\text{ right guess}]} \\\\ &amp;+ \\frac{1}{2}Pr{[\\mathcal{A}\\text{ computes } B(\\alpha, x) | \\text{ wrong guess}]} \\tag{5.2}\\\\ &amp;=\\frac{1}{2}Pr{[D(\\dots, B(\\alpha, x)) = 1]} \\\\ &amp;+ \\frac{1}{2}Pr{[D(\\dots, B(\\alpha, x) \\oplus 1) = 0]} \\tag{5.3}\\\\ &amp;= \\frac{1}{2}Pr{[D(\\dots, B(\\alpha, x)) = 1]} \\\\ &amp;+ \\frac{1}{2}(1 - Pr{[D(\\dots, B(\\alpha, x) \\oplus 1) = 1]})\\\\ &amp;=\\frac{1}{2} + \\frac{1}{2}\\mathcal{D}(\\beta_0, \\beta_1) \\\\ &amp;\\geq \\frac{1}{2} + \\frac{1}{2p(n)} \\tag{5.4} \\end{align}\\] (5.2) comes from Bayes rule and that \\(\\mathcal{A}\\) guesses \\(\\beta_{1 - \\sigma}\\) correctly with probability 1/2. (5.3) comes from the assumption made about distinguishers. Finally (5.4) comes from the assumption in (5.1) What we have shown is that \\(\\mathcal{A}\\) can just invoke \\(D\\) to guess the hard-core predicate of a trapdoor permutation with probability non negligibly greater than \\(\\frac{1}{2}\\). This contradicts our assumption about the existence of hard-core predicates and one way functions, therefore the assumption in (5.1) must have been wrong. That completes the proof We remark that this protocol is a good example of the fact that security in the presence of semi-honest adversaries guarantees nothing if the corrupted party does not behave completely honestly. In particular, if \\(P_2\\) generates both \\(y_0\\) and \\(y_1\\) by choosing \\(x_0\\), \\(x_1\\) and computing \\(y_0 = F(\\alpha, x_0)\\) and \\(y_1 = F =(\\alpha, x_1)\\), then it will learn both \\(b_0\\) and \\(b_1\\). Furthermore, \\(P_1\\) has no way of detecting this at all. 5.3 Static Malicious PPT Adversaries Next we consider more realistic adversaries that do abide by the rules set by a protocol. Unlike the semi-honest world, the simulator must be able to extract the inputs from the adverasary – as we do not know apriori what inputs they will use. Note that the simulator cannot talk to the honest party. It can only externally interact with the trusted party and internally with the real world adversary by pretending to be the honest party. Thus to be able to generate the same view as the real world adversary who has access to messages from the honest party, we must give the simulator some extra power. This special power is that of rewinding the adversary. It can send the adversary \\(\\mathcal{A}\\) some input to get it to generate some output, and then delete \\(\\mathcal{A}\\)’s memory completely so that \\(\\mathcal{A}\\) has no idea what happened. This trick will prove very useful in proving security in the malicious setting. 5.4 Covert OT Shown above is the protocol that is secure in presence of covert adversaries with explicit cheat (as described in (Aumann and Lindell 2010) References "],["shuffle-privacy.html", " 6 Shuffle Privacy", " 6 Shuffle Privacy #differential_privacy My notes on shuffle privacy. "],["the-hybrid-argument.html", " 7 The Hybrid Argument", " 7 The Hybrid Argument cryptography dp Shows up everywhere in every document I read. For example in Chapter 1 of (Vadhan 2017) we see the following quote Then by a hybrid argument, for all pairs of datasets \\(X, X^\\prime \\in \\mathcal{X}^n\\) (even non-neighbors), we have \\(SD(M(X),M(X^\\prime)) ≤ n\\delta \\leq 1/2\\). Taking X^to be a fixed (e.g. all-zeroes) dataset, this means that with probability 1/2 on \\(M(x)\\), we get an answer independent of the dataset \\(X\\) and the mechanism is useless. What is the hybrid argument? References "],["solutions-foundations-of-cryptography-preliminaries.html", " 8 (Solutions) Foundations of Cryptography: Preliminaries 8.1 Problem 1 8.2 Problem 2 8.3 Problem 3 8.4 Problem 4 8.5 Problem 5 8.6 Problem 6", " 8 (Solutions) Foundations of Cryptography: Preliminaries Solutions to exercises in Chapter 1 of (Goldreich 2004) 8.1 Problem 1 Let \\(X\\) be a random variable such that \\(\\mathbb{E}(X) = \\mu\\) and \\(X \\leq 2\\mu\\). Give an upper bound on \\(Pr[X \\leq \\frac{\\mu}{2}]\\). A little transformation of random variables to get a non negative RV. Define \\(Y = 2\\mu - X \\geq 0\\) \\[\\begin{align*} P[X \\leq \\frac{\\mu}{2}] &amp;= P[2\\mu - Y \\leq \\frac{\\mu}{2}] \\\\ &amp;= P[Y \\geq 2\\mu - \\frac{\\mu}{2}] \\\\ &amp;\\leq \\frac{E[Y]}{1.5\\mu} \\tag{8.1} \\\\ &amp;= \\frac{2}{3} \\end{align*}\\] (8.1) By Markov 8.2 Problem 2 Let \\(0 &lt; \\epsilon\\) and \\(\\delta &lt; 1\\), and let \\(Y\\) be a random variable ranging in the interval \\([0,1]\\) such that \\(\\mathbb{E}(Y) = \\delta + \\epsilon\\). Give a lower bound on \\(Pr[Y \\geq \\delta + 2\\epsilon ]\\). Another auxilliary transformation is needed but we will use the proof style of markov as recommended in the guide. First note if \\(\\delta + \\frac{\\epsilon}{2} &gt; 1\\), then \\(P[Y \\geq \\delta + \\frac{\\epsilon}{2}] = 0\\) since \\(Y \\in [0,1]\\). So we can assume \\(\\delta + \\frac{\\epsilon}{2} \\leq 1\\). Similarly, if \\(\\delta + \\frac{\\epsilon}{2} &lt; 0\\), then \\(P[Y \\geq \\delta + \\frac{\\epsilon}{2}] = 1\\) \\[\\begin{align*} E[Y] &amp;= \\int_{0}^1 yf(y)dy \\\\ &amp;= \\int_{0}^{\\delta + \\frac{\\epsilon}{2}} yf(y)dy + \\int_{\\delta + \\frac{\\epsilon}{2}}^1 yf(y)dy \\\\ &amp;\\geq \\int_{\\delta + \\frac{\\epsilon}{2}}^1 yf(y)dy \\\\ &amp;\\geq \\delta + \\frac{\\epsilon}{2}\\int_{\\delta + \\frac{\\epsilon}{2}}^1 f(y)dy \\\\ \\end{align*}\\] Therefore, \\(P[Y \\geq \\delta + \\frac{\\epsilon}{2}] \\leq \\frac{\\epsilon + \\delta}{\\delta + \\frac{\\epsilon}{2}}\\). 8.3 Problem 3 Consider the following algorithm: On input a graph \\(G = (V, E)\\) and two vertices, \\(s\\) and \\(t\\), we take a random walk of length \\(O(|V | · |E|)\\), starting at vertex \\(s\\), and test at each step whether or not vertex \\(t\\) is encountered. If vertex \\(t\\) is ever encountered, then the algorithm will accept; otherwise, it will reject. By a random walk we mean that at each step we uniformly select one of the edges incident at the current vertex and traverse this edge to the other endpoint. *show that if \\(s\\) is connected to \\(t\\) in the graph \\(G\\), then, with probability at least \\(\\frac{2}{3}\\), vertex \\(t\\) will be encountered in a random walk starting at \\(s\\). I have not yet been able to solve this problem. TODO with Daniel. 8.4 Problem 4 Chernoff Bound Problem This comes quite directly from Hoeffding’s inequality. \\(A\\) samples \\(s_1, s_2, \\dots, s_{p(n)}\\) where \\(s_i \\sim Uniform(\\{0, 1\\}^n)\\) and computes \\(f(s_1), \\dots, f(s_{p(n)})\\) which are identical i.i.d random variables each bounded in \\([0,1]\\). Let \\(A_{p(n)} = f(s_1) + \\dots + f(s_{p(n)})\\) By Hoeffding: \\[\\begin{align*} P\\Big[ | E[A_{p(n)}] - A_{p(n)} | \\geq 1\\Big] \\leq 2^{-n} \\end{align*}\\] Log bases can be easily converted from one to the other, in this document all bases are \\(\\log_2\\). 8.5 Problem 5 Equivalent definition of BPP. Part 1: Prove that Definition of \\(\\mathcal{BPP}\\) is robust when \\(\\frac{2}{3}\\) is replaced by \\(\\frac{1}{2} + \\frac{1}{p(|x|)}\\) for every positive polynomial \\(p(·)\\). Namely, show that \\(L \\in \\mathcal{BPP}\\) if there exists a polynomial \\(p(·)\\) and a probabilistic polynomial-time machine \\(M\\) such that for every \\(x \\in L\\) it holds that \\(P[M(x)=1] \\geq \\frac{1}{2} + \\frac{1}{p(|x|)}\\) Non false positiveness: for every \\(x \\notin L\\) it holds that \\(P[M(x)=0] \\geq \\frac{1}{2} + \\frac{1}{p(|x|)}\\) Same idea as above but using Chebychev’s instead of Chernoff. Did not want to write latex so uploading picture of solution. 8.6 Problem 6 Now do the same again: but replace \\(\\frac{2}{3}\\) by \\(1 + \\frac{1}{2^{p(|x|)}}\\) Used as aid Assume \\(L \\in \\mathcal{BPP}\\), this implies that there exists some probabilistic polynomial time turing macine \\(M\\) such that \\(P\\Big[ M(x) = 1 | x \\in L \\Big] \\geq 2/3\\). Define \\(r_1, \\dots, r_{t(n)}\\) as randomv variables sampled uniformly at random from \\(U^n\\). For the sake convenience we will write \\(t(n) = t\\). Define \\(X_i = 1\\) if \\(M_{r_i}(x) = I[x \\in L]\\). Simply put if \\(X_i=1\\), then \\(M_{r}\\) predicted correctly. Let \\(S_t = X_1 + \\dots + X_{r_t}\\). Define a turing machine \\(\\hat{M}\\) such that \\(\\hat{M}(x) = 1\\) if \\(S_t \\geq \\frac{t}{2}\\). In other words, \\(\\hat{M}\\) selects the majority from the predictions of the random machines. Note each \\(X_i\\) is a bernoulli random variable with \\(P\\Big[X_i = 1 | x\\in L\\Big] = p \\geq \\frac{2}{3}\\) Reminder, the chernoff bound, for a sum of \\(t\\) bernoulli variables with mean \\(q\\) is \\[\\begin{align*} P[S_t \\leq (1 - \\delta)qt] \\leq \\text{exp}\\{- \\frac{\\delta^2p}{2}\\} \\end{align*}\\] Now \\(\\hat{M}\\) makes a mistake if \\(x \\in L\\) but \\(S_t \\leq t/2\\) \\[\\begin{align*} P[\\hat{M} \\text{ makes a mistake}] &amp;= P\\Big[ S_t \\leq \\frac{t}{2} \\text{ | } x \\in L \\Big] \\\\ &amp;= P\\Big[ S_t \\leq (1 - (1 - \\frac{1}{2p}))pt \\text{ | } x \\in L \\Big] \\\\ &amp;\\leq \\exp\\{- t\\frac{(p - 1/2)^2}{2p}\\} \\\\ &amp;= \\exp\\{- f(p)\\frac{t}{2}\\} \\tag{8.2} \\end{align*}\\] (8.2) \\(f(p) = \\frac{(p - 1/2)^2}{p}\\) is an increasing function in \\((1/2, \\infty)\\), therefore \\(e^{-f(p)}\\) is decreasing. We have \\(p \\geq 2/3\\), therefore \\(e^{-f(p)} \\leq e^{-f(2/3)} = e^{-\\frac{1}{24}}\\). Using this, \\[\\begin{align*} P[\\hat{M} \\text{ makes a mistake}] &amp;= P\\Big[ S_t \\leq \\frac{t}{2} \\text{ | } x \\in L \\Big] \\\\ &amp;= \\exp\\{- f(p)\\frac{t}{2}\\} \\\\ &amp;= \\exp\\{- \\frac{1}{24}\\frac{t}{2}\\} \\\\ &amp;\\leq 2^{-p(n)} \\end{align*}\\] If \\(t(n) \\geq (48 \\ln(2))p(n) \\approx O(n)\\), then \\(\\hat{M}\\) is poly turing machine that does the job The other side is trivial, in that, pick \\(p(n) \\geq -\\ln(1/3)\\) and assume \\(P\\Big[ M(x) = 1 | x \\in L \\Big] \\geq 1 - 2^{-p(n)}\\). Then \\(P\\Big[ M(x) = 1 | x \\in L \\Big] \\geq \\frac{2}{3}\\). References "],["solutions-foundations-of-cryptography-one-way-functions.html", " 9 (Solutions) Foundations of Cryptography: One Way Functions", " 9 (Solutions) Foundations of Cryptography: One Way Functions "],["solutions-foundations-of-cryptography-pseudo-randomness.html", " 10 (Solutions) Foundations of Cryptography: Pseudo randomness", " 10 (Solutions) Foundations of Cryptography: Pseudo randomness "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
