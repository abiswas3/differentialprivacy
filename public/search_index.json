[["index.html", "My Notes 1 My notes", " My Notes Ari 2022-06-16 1 My notes This page is a repository of notes I’ve written to teach myself some mathematics "],["intro.html", " 2 Notes on DP", " 2 Notes on DP #differential_privacy You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2022) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) ad (Ghazi et al. 2019) References "],["smooth-distributions.html", " 3 Smooth Distributions", " 3 Smooth Distributions #differential_privacy We are interested in releasing counting queries \\(q\\) that are \\(k\\)-incremental Definition 3.1 (K-incremental functions) (Ghazi et al. 2019) A function \\(q: \\mathcal{X}^n \\rightarrow \\mathbb{Z}^M\\) is said to be \\(k\\)-incremental if for all neighbouring datasets \\(X \\sim X^{\\prime}\\), \\(|| f(X) - f(X^{\\prime})||_{\\infty} \\leq k\\). Theorem 3.1 (Smooth Distributions guarantee DP) Suppose \\(q: \\mathcal{X}^n \\rightarrow \\mathbb{Z}^M\\) is \\(k\\)-incremental i.e. for all neighbouring datasets \\(X \\sim X^{\\prime}\\) we have \\(||q(X) - q(X^{\\prime}) ||_{\\infty} = k\\) and \\(\\Delta(q) = || q(X) - q(X^{\\prime}) ||_1 = \\Delta\\). Let \\(\\mathcal{D}\\) be a \\((\\varepsilon, \\delta, k)\\)-smooth distribution. Then the mechanism \\(M\\) \\[\\begin{equation*} M_{(Y_1, \\dots, Y_M)}(X, q) = q(X) + (Y_1, \\dots, Y_M) \\end{equation*}\\] is \\((\\epsilon\\Delta, \\delta\\Delta)\\) differentially private, where \\((Y_1, \\dots, Y_M) \\overset{\\text{i.i.d}}{\\sim} D.\\) Proof. Let \\(X = (x_1, \\dots, x_n)\\) where \\(x_i \\in \\mathcal{X}\\) and \\(X^\\prime = (x_1, \\dots, x_n^\\prime)\\). Let \\(\\vec{y} = (y_1, \\dots, y_M)\\) and \\(\\vec{Y} = (Y_1, \\dots, Y_M)\\) be i.i.d draws from \\(\\mathcal{D}\\). Assume that Equation (3.1) holds \\[\\begin{equation} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{[g(\\vec{y}) \\geq e^{\\varepsilon^\\prime}]} \\leq \\delta^\\prime \\tag{3.1} \\end{equation}\\] where \\(g(\\vec{y}) = \\frac{\\Pr_{(\\vec{Y} \\sim \\mathcal{D})}{[M_{\\vec{Y}}(X, q) = q(X) + \\vec{y} ]}}{\\Pr_{(\\vec{Y} \\sim \\mathcal{D})}{[M_{\\vec{Y}}(X^\\prime, q) = q(X^\\prime) + \\vec{y} ]}}\\). Let \\(S \\subseteq \\mathbb{Z}^M\\) be an arbitrary subset in the range of \\(M\\). Let \\(T = \\{M_{\\vec{y}(X, q)} | g(\\vec{y}) &lt; e^{\\varepsilon^\\prime}\\}\\) represent a set of outputs of \\(M\\) over draws of \\(\\vec{y}\\) such that \\(g(\\vec{y}) &lt; e^{\\varepsilon^\\prime}\\). Then from equation (3.1) we can show that \\(M\\) is \\((\\varepsilon^\\prime, \\delta^\\prime)\\) differentially private \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X, q) \\in S]} &amp;\\leq \\delta^\\prime + \\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X, q) \\in S \\cap T]} \\tag{3.2}\\\\ &amp;= \\delta^\\prime + \\sum_{w \\in S \\cap T }\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X, q) = w]} \\\\ &amp;\\leq \\delta^\\prime + \\sum_{w \\in S \\cap T }e^{\\varepsilon^\\prime}\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X^\\prime, q) = w]} \\tag{3.3}\\\\ &amp;\\leq \\delta^\\prime + \\sum_{w \\in S}e^{\\varepsilon^\\prime}\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X^\\prime, q) = w]} \\tag{3.4}\\\\ &amp;= \\delta^\\prime + e^{\\varepsilon^\\prime}\\Pr_{\\vec{y} \\sim \\mathcal{D}}{[M_{\\vec{y}}(X^\\prime, q) \\in S]} \\end{align}\\] Equation (3.2) is from the law of total probability, equation (3.2) comes from equation (3.1) assumption and equation (3.4) is true as \\(T \\cap S \\subseteq S\\). Therefore all there is to do is to show that equation (3.1) is true if \\(\\mathcal{D}\\) is as defined and \\(\\varepsilon^\\prime = \\varepsilon\\Delta, \\delta^\\prime= \\delta\\Delta\\) to complete the proof. Define \\(k_j = q(X)_j - q(X^\\prime)_j\\). As each coordinate of \\(q(X)\\) is independently perturbed and \\(q\\) is a deterministic function, equation @ref(eq: smooth-dp) is equivalent to equation (3.5). \\[\\begin{equation} \\tag{3.5} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\prod_{j=1}^M \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{\\varepsilon^\\prime}\\Bigg]} \\leq \\delta^\\prime \\end{equation}\\] Thus, in order to prove equation (3.1) is true, it suffices to show that equation (3.5) holds. We know that \\(\\mathcal{D}\\) is a smooth distribution i.e for each \\(j \\in [M]\\) \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{|k_j|\\varepsilon}\\Bigg]} \\leq \\delta \\end{align}\\] We can apply the union bound to get the probability that the joint distribution over all indices to get \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\prod_{j=1}^M \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{\\sum_{j=1}^m|k_j|\\varepsilon}\\Bigg]} \\leq \\delta \\sum_{j=1}^M \\mathbb{I}(k_j \\neq 0) \\end{align}\\] Given the sensitivity of \\(q\\) is \\(\\Delta\\), at most \\(\\Delta\\) indices for \\(k_j\\) can be non zero and \\((\\sum_{j=1}^M |k_j|) = \\Delta\\). Finally we get the result we want \\[\\begin{align} \\Pr_{\\vec{y} \\sim \\mathcal{D}}{\\Bigg[ \\frac{\\Pr_{(Y_j \\sim \\mathcal{D})}{\\prod_{j=1}^M [Y_j = y_j ]}}{\\Pr_{(Y_j \\sim \\mathcal{D})}{[Y_j = y_j + k_j ]}} \\geq e^{\\Delta\\varepsilon}\\Bigg]} \\leq \\delta \\Delta \\label{eq: non_zero} \\end{align}\\] Ahoy 3.1 Lemma 3.1 Let \\(n \\in \\mathbb{N}\\), \\(p \\in [0, 1/2]\\), \\(\\alpha \\in [0,1)\\) and \\(k \\leq \\frac{n\\alpha p}{2}\\). Then the binomial distribution \\(Bin(n, p)\\) is a \\((\\varepsilon, \\delta, k)\\)-smooth distribution. Proof. Let \\(Y \\sim Bin(np)\\), then \\(\\Pr{[Y = y]} = {n \\choose y}p^{y}(1-p)^{n-y}\\). For any \\(-k \\leq k^\\prime \\leq k\\), define an interval \\(\\mathcal{\\varepsilon} := [(1 - \\alpha)np + k^\\prime, (1 + \\alpha)np - k^\\prime]\\). This an interval of size \\(k\\) around the mean of the distribution. Note that as long as \\(k \\leq \\frac{np}{2}\\alpha\\), then the interval \\(\\mathcal{\\varepsilon^\\prime} := [(1 - \\alpha/2)np, (1 + \\alpha/2)np]\\) is contained inside of \\(\\mathcal{\\varepsilon}\\). Thus if \\(y \\sim Bin(np)\\) is not in \\(\\mathcal{\\varepsilon}\\), it is also not inside \\(\\mathcal{\\varepsilon^\\prime}\\). We know how to bound the probability that \\(y \\notin \\mathcal{\\varepsilon^\\prime}\\) by using the multiplicative chernoff bound. Invoking it, we get \\[\\begin{align} \\Pr_{y \\sim Bin(n,p)}{[y \\notin \\epsilon]} &amp;\\leq \\Pr_{y \\sim Bin(n,p)}{[y \\notin \\epsilon^\\prime]} \\\\ &amp;\\leq e^{-\\frac{-\\alpha^2np}{8}} + e^{-\\frac{-\\alpha^2np}{8+2\\alpha}} \\\\ &amp;= \\delta \\\\ \\end{align}\\] Now for all \\(y \\in \\mathcal{\\varepsilon}\\), we have for \\(0 \\leq k^\\prime \\leq k\\) \\[\\begin{align} \\frac{\\Pr{[Y = y]}}{\\Pr{[Y = y + k^\\prime]}} &amp;= \\Big(\\frac{1-p}{p}\\Big)^{k^\\prime} \\prod_{i=1}^{k^\\prime} \\frac{y + i}{n-y -i+1} \\\\ &amp;\\leq \\Big(\\frac{1-p}{p}\\Big)^{k^\\prime} \\Big(\\frac{y + k^\\prime}{n-y -k^\\prime}\\Big)^{k^\\prime} \\\\ &amp;\\leq \\Big(\\frac{1-p}{p}\\Big)^{k^\\prime} \\Big(\\frac{(1 + \\alpha)np}{n- (1 + \\alpha)np}\\Big)^{k^\\prime} \\tag{3.6}\\\\ &amp;= (1 + \\alpha)^{k^\\prime} \\Big(\\frac{1-p}{1- p - p\\alpha}\\Big)^{k^\\prime} \\\\ \\end{align}\\] TODO Ari/Graham fix the the last inequality (3.6) comes from our assumption \\(y \\in \\mathcal{\\varepsilon}\\) and so when \\(y=(1 + \\alpha)np - k^\\prime\\) the ratio above is the largest. Similarly for \\(-k \\leq k^\\prime \\leq 0\\) we have \\[\\begin{align} \\frac{\\Pr{[Y = y]}}{\\Pr{[Y = y + |k^\\prime |]}} &amp;= \\Big(\\frac{p}{1-p}\\Big)^{|k^\\prime|} \\prod_{i=1}^{|k^\\prime|} \\frac{n-y +i}{y - i + 1} \\\\ &amp;\\leq \\Big(\\frac{p}{1-p}\\Big)^{|k^\\prime|} \\Big( \\frac{n-y +|k^\\prime|}{y - |k^\\prime|}\\Big)^{|k^\\prime|} \\\\ &amp;\\leq \\Big( \\frac{1 + p\\alpha - p}{(1-\\alpha)(1 - p)}\\Big)^{|k^\\prime|} \\label{eq: plug_in} \\\\ &amp;\\leq \\Big(\\frac{1 + \\alpha}{1 - \\alpha}\\Big)^{|k^\\prime|}\\tag{3.7} \\\\ &amp;= e^{|k|\\varepsilon} \\end{align}\\] (3.6) comes from plugging in the smallest value for \\(y\\) and (3.7) comes from the fact that \\(p \\leq 1/2\\) Finally we can prove smoothness using bayes rule. Let \\(g(y) = \\frac{Pr_{Y \\sim Bin(n,p)}[Y = y]}{Pr_{Y \\sim Bin(n, p)}[Y = y+k^{\\prime}]}\\) \\[\\begin{align} Pr_{y \\sim Bin(n, p)} \\Big[ g(y) \\geq e^{|k^{\\prime}|\\epsilon}\\Big] &amp;\\leq Pr_{y \\sim Bin(n, p)} \\Big[ g(y) \\geq e^{|k^{\\prime}|\\epsilon} \\Big| y \\notin \\mathcal{\\varepsilon} \\Big] + \\delta \\\\ &amp;\\leq e^{-\\frac{-\\alpha^2np}{8}} + e^{-\\frac{-\\alpha^2np}{8+2\\alpha}} \\tag{3.8} \\\\ &amp;\\leq \\delta \\end{align}\\] (3.8) comes from the fact that when \\(y \\in \\mathcal{\\varepsilon}\\) we have \\(\\frac{\\Pr{[Y = y]}}{\\Pr{[Y = y + k^\\prime]}} \\leq e^{\\varepsilon}|k^\\prime|\\) and by how we defined \\(\\varepsilon\\) in equation (3.7). So we have \\(Pr_{y \\sim Bin(n, p)} \\Bigg[ \\frac{Pr_{Y \\sim Bin(n,p)}[Y = y]}{Pr_{Y \\sim Bin(n, p)}[Y = y+k^{\\prime}]} \\geq e^{|k^{\\prime}|\\epsilon} \\Big| y \\notin \\mathcal{\\varepsilon} \\Bigg] = 0\\) References "],["beavers-triples.html", " 4 Beavers Triples", " 4 Beavers Triples #cryptography #mpc Todo SNIPS notes "],["simulation.html", " 5 Simulation 5.1 Semi-honest OT 5.2 Malicious OT 5.3 Covert OT", " 5 Simulation #cryptography #mpc 5.1 Semi-honest OT 5.2 Malicious OT 5.3 Covert OT "],["shuffle-privacy.html", " 6 Shuffle Privacy", " 6 Shuffle Privacy #differential_privacy My notes on shuffle privacy. "],["the-hybrid-argument.html", " 7 The Hybrid Argument", " 7 The Hybrid Argument cryptography dp Shows up everywhere in every document I read. For example in Chapter 1 of (Vadhan 2017) we see the following quote Then by a hybrid argument, for all pairs of datasets \\(X, X^\\prime \\in \\mathcal{X}^n\\) (even non-neighbors), we have \\(SD(M(X),M(X^\\prime)) ≤ n\\delta \\leq 1/2\\). Taking X^to be a fixed (e.g. all-zeroes) dataset, this means that with probability 1/2 on \\(M(x)\\), we get an answer independent of the dataset \\(X\\) and the mechanism is useless. What is the hybrid argument? References "],["solutions-foundations-of-cryptography-preliminaries.html", " 8 (Solutions) Foundations of Cryptography: Preliminaries 8.1 Problem 1 8.2 Problem 2 8.3 Problem 3 8.4 Problem 4 8.5 Problem 5 8.6 Problem 6", " 8 (Solutions) Foundations of Cryptography: Preliminaries Solutions to exercises in Chapter 1 of (Goldreich 2004) 8.1 Problem 1 Let \\(X\\) be a random variable such that \\(\\mathbb{E}(X) = \\mu\\) and \\(X \\leq 2\\mu\\). Give an upper bound on \\(Pr[X \\leq \\frac{\\mu}{2}]\\). A little transformation of random variables to get a non negative RV. Define \\(Y = 2\\mu - X \\geq 0\\) \\[\\begin{align*} P[X \\leq \\frac{\\mu}{2}] &amp;= P[2\\mu - Y \\leq \\frac{\\mu}{2}] \\\\ &amp;= P[Y \\geq 2\\mu - \\frac{\\mu}{2}] \\\\ &amp;\\leq \\frac{E[Y]}{1.5\\mu} \\tag{8.1} \\\\ &amp;= \\frac{2}{3} \\end{align*}\\] (8.1) By Markov 8.2 Problem 2 Let \\(0 &lt; \\epsilon\\) and \\(\\delta &lt; 1\\), and let \\(Y\\) be a random variable ranging in the interval \\([0,1]\\) such that \\(\\mathbb{E}(Y) = \\delta + \\epsilon\\). Give a lower bound on \\(Pr[Y \\geq \\delta + 2\\epsilon ]\\). Another auxilliary transformation is needed but we will use the proof style of markov as recommended in the guide. First note if \\(\\delta + \\frac{\\epsilon}{2} &gt; 1\\), then \\(P[Y \\geq \\delta + \\frac{\\epsilon}{2}] = 0\\) since \\(Y \\in [0,1]\\). So we can assume \\(\\delta + \\frac{\\epsilon}{2} \\leq 1\\). Similarly, if \\(\\delta + \\frac{\\epsilon}{2} &lt; 0\\), then \\(P[Y \\geq \\delta + \\frac{\\epsilon}{2}] = 1\\) \\[\\begin{align*} E[Y] &amp;= \\int_{0}^1 yf(y)dy \\\\ &amp;= \\int_{0}^{\\delta + \\frac{\\epsilon}{2}} yf(y)dy + \\int_{\\delta + \\frac{\\epsilon}{2}}^1 yf(y)dy \\\\ &amp;\\geq \\int_{\\delta + \\frac{\\epsilon}{2}}^1 yf(y)dy \\\\ &amp;\\geq \\delta + \\frac{\\epsilon}{2}\\int_{\\delta + \\frac{\\epsilon}{2}}^1 f(y)dy \\\\ \\end{align*}\\] Therefore, \\(P[Y \\geq \\delta + \\frac{\\epsilon}{2}] \\leq \\frac{\\epsilon + \\delta}{\\delta + \\frac{\\epsilon}{2}}\\). 8.3 Problem 3 Consider the following algorithm: On input a graph \\(G = (V, E)\\) and two vertices, \\(s\\) and \\(t\\), we take a random walk of length \\(O(|V | · |E|)\\), starting at vertex \\(s\\), and test at each step whether or not vertex \\(t\\) is encountered. If vertex \\(t\\) is ever encountered, then the algorithm will accept; otherwise, it will reject. By a random walk we mean that at each step we uniformly select one of the edges incident at the current vertex and traverse this edge to the other endpoint. *show that if \\(s\\) is connected to \\(t\\) in the graph \\(G\\), then, with probability at least \\(\\frac{2}{3}\\), vertex \\(t\\) will be encountered in a random walk starting at \\(s\\). I have not yet been able to solve this problem. TODO with Daniel. 8.4 Problem 4 Chernoff Bound Problem This comes quite directly from Hoeffding’s inequality. \\(A\\) samples \\(s_1, s_2, \\dots, s_{p(n)}\\) where \\(s_i \\sim Uniform(\\{0, 1\\}^n)\\) and computes \\(f(s_1), \\dots, f(s_{p(n)})\\) which are identical i.i.d random variables each bounded in \\([0,1]\\). Let \\(A_{p(n)} = f(s_1) + \\dots + f(s_{p(n)})\\) By Hoeffding: \\[\\begin{align*} P\\Big[ | E[A_{p(n)}] - A_{p(n)} | \\geq 1\\Big] \\leq 2^{-n} \\end{align*}\\] Log bases can be easily converted from one to the other, in this document all bases are \\(\\log_2\\). 8.5 Problem 5 Equivalent definition of BPP. Part 1: Prove that Definition of \\(\\mathcal{BPP}\\) is robust when \\(\\frac{2}{3}\\) is replaced by \\(\\frac{1}{2} + \\frac{1}{p(|x|)}\\) for every positive polynomial \\(p(·)\\). Namely, show that \\(L \\in \\mathcal{BPP}\\) if there exists a polynomial \\(p(·)\\) and a probabilistic polynomial-time machine \\(M\\) such that for every \\(x \\in L\\) it holds that \\(P[M(x)=1] \\geq \\frac{1}{2} + \\frac{1}{p(|x|)}\\) Non false positiveness: for every \\(x \\notin L\\) it holds that \\(P[M(x)=0] \\geq \\frac{1}{2} + \\frac{1}{p(|x|)}\\) Same idea as above but using Chebychev’s instead of Chernoff. Did not want to write latex so uploading picture of solution. 8.6 Problem 6 Now do the same again: but replace \\(\\frac{2}{3}\\) by \\(1 + \\frac{1}{2^{p(|x|)}}\\) Used as aid Assume \\(L \\in \\mathcal{BPP}\\), this implies that there exists some probabilistic polynomial time turing macine \\(M\\) such that \\(P\\Big[ M(x) = 1 | x \\in L \\Big] \\geq 2/3\\). Define \\(r_1, \\dots, r_{t(n)}\\) as randomv variables sampled uniformly at random from \\(U^n\\). For the sake convenience we will write \\(t(n) = t\\). Define \\(X_i = 1\\) if \\(M_{r_i}(x) = I[x \\in L]\\). Simply put if \\(X_i=1\\), then \\(M_{r}\\) predicted correctly. Let \\(S_t = X_1 + \\dots + X_{r_t}\\). Define a turing machine \\(\\hat{M}\\) such that \\(\\hat{M}(x) = 1\\) if \\(S_t \\geq \\frac{t}{2}\\). In other words, \\(\\hat{M}\\) selects the majority from the predictions of the random machines. Note each \\(X_i\\) is a bernoulli random variable with \\(P\\Big[X_i = 1 | x\\in L\\Big] = p \\geq \\frac{2}{3}\\) Reminder, the chernoff bound, for a sum of \\(t\\) bernoulli variables with mean \\(q\\) is \\[\\begin{align*} P[S_t \\leq (1 - \\delta)qt] \\leq \\text{exp}\\{- \\frac{\\delta^2p}{2}\\} \\end{align*}\\] Now \\(\\hat{M}\\) makes a mistake if \\(x \\in L\\) but \\(S_t \\leq t/2\\) \\[\\begin{align*} P[\\hat{M} \\text{ makes a mistake}] &amp;= P\\Big[ S_t \\leq \\frac{t}{2} \\text{ | } x \\in L \\Big] \\\\ &amp;= P\\Big[ S_t \\leq (1 - (1 - \\frac{1}{2p}))pt \\text{ | } x \\in L \\Big] \\\\ &amp;\\leq \\exp\\{- t\\frac{(p - 1/2)^2}{2p}\\} \\\\ &amp;= \\exp\\{- f(p)\\frac{t}{2}\\} \\tag{8.2} \\end{align*}\\] (8.2) \\(f(p) = \\frac{(p - 1/2)^2}{p}\\) is an increasing function in \\((1/2, \\infty)\\), therefore \\(e^{-f(p)}\\) is decreasing. We have \\(p \\geq 2/3\\), therefore \\(e^{-f(p)} \\leq e^{-f(2/3)} = e^{-\\frac{1}{24}}\\). Using this, \\[\\begin{align*} P[\\hat{M} \\text{ makes a mistake}] &amp;= P\\Big[ S_t \\leq \\frac{t}{2} \\text{ | } x \\in L \\Big] \\\\ &amp;= \\exp\\{- f(p)\\frac{t}{2}\\} \\\\ &amp;= \\exp\\{- \\frac{1}{24}\\frac{t}{2}\\} \\\\ &amp;\\leq 2^{-p(n)} \\end{align*}\\] If \\(t(n) \\geq (48 \\ln(2))p(n) \\approx O(n)\\), then \\(\\hat{M}\\) is poly turing machine that does the job The other side is trivial, in that, pick \\(p(n) \\geq -\\ln(1/3)\\) and assume \\(P\\Big[ M(x) = 1 | x \\in L \\Big] \\geq 1 - 2^{-p(n)}\\). Then \\(P\\Big[ M(x) = 1 | x \\in L \\Big] \\geq \\frac{2}{3}\\). References "],["solutions-foundations-of-cryptography-one-way-functions.html", " 9 (Solutions) Foundations of Cryptography: One Way Functions", " 9 (Solutions) Foundations of Cryptography: One Way Functions "],["solutions-foundations-of-cryptography-pseudorandomness.html", " 10 (Solutions) Foundations of Cryptography: Pseudorandomness", " 10 (Solutions) Foundations of Cryptography: Pseudorandomness "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
