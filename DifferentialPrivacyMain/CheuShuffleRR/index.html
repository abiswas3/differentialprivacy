<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Readme</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../blog.css">
  <script src="../../code/es5/tex-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/code/bootstrap.min.css">
  <script src="/code/jquery.min.js"></script>
  <script src="/code/bootstrap.min.js"></script>
  
  
  <ul class="bar">
    <li class="barli"><a class="active" href="/">Home</a></li>
    <li class="barli"><a href="/differentialprivacy/index.html">My notes</a></li>  
    <li class="barli"><a href="./..">Back</a></li>    
    <!-- <li class="barli"><a href="https://abiswas3.github.io/differentialprivacy.github.io">Notes on Differential Privacy</a></li> -->
  </ul>
</head>
<body>

<div class="container">
<h1 id="distributed-differential-privacy-via-shuffling">Distributed Differential Privacy via Shuffling</h1>
<p>Year published: 2018 Conference:</p>
<p><u><em>Why:</em></u> Each member of the dataset <span class="math inline">\(x \in D\)</span> holds a value in <span class="math inline">\(\{0, 1\}\)</span>. We simply want to estimate <span class="math inline">\(f(X) = \sum_{i=1}^n x_i\)</span>. Let <span class="math inline">\(\hat{f}(X)\)</span> be the estimated sum by the algorithm. This is the simplest version of multiplicative noise. If we assume that the population is not too sparse, then it is almost identical to sample and threshold with Poisson sampling. The difference being in randomised response when we see tails we send a 1 or 0 with probability 1/2. Sample and threshold privacy sends 0 deterministically on seeing tails.</p>
<h2 id="main-results">Main Results</h2>
<ol type="1">
<li>For every <span class="math inline">\(\epsilon \in (0, 1)\)</span>, and every <span class="math inline">\(\delta &gt; 2^{−\epsilon n}\)</span> and every function <span class="math inline">\(f : X \rightarrow \{0, 1\}\)</span>, there is a protocol <span class="math inline">\(P\)</span> in the shuffled model that is <span class="math inline">\((\epsilon, \delta)\)</span>-differentially private, and for every <span class="math inline">\(n\)</span> and every <span class="math inline">\(X = (x_1,...,x_n) \in \mathcal{X}^n\)</span>,</li>
</ol>
<span class="math display">\[\begin{align}
\mathbb{E}\Bigg[ \Big|P(X) - \sum_{i=1}^n f(x_i)\Big|\Bigg] &amp;= O\Bigg( \frac{1}{\epsilon}\sqrt{\ln \frac{1}{\delta}}\Bigg)
\end{align}\]</span>
<p><strong>Each user sends a single one-bit message.</strong></p>
<div class="important">
<p>This is saying that for binary sums, if I did randomised response then I find a protocol such that my shuffle privacy estimate will be within a constant of the actual sum.</p>
<p>The protocol I find is simply randomised response. Remember upper bound don’t need for all algorithms, just one you devise.</p>
</div>
<p><br></p>
<ol start="2" type="1">
<li>For every <span class="math inline">\(\epsilon \in (0, 1)\)</span>, and every <span class="math inline">\(\delta &gt; \epsilon n 2^{−\epsilon n}\)</span> and every function <span class="math inline">\(f : X \rightarrow [0, 1]\)</span>, there is a protocol <span class="math inline">\(P\)</span> in the shuffled model that is <span class="math inline">\((\epsilon, \delta)\)</span>-differentially private, and for every <span class="math inline">\(n\)</span> and every <span class="math inline">\(X = (x_1,...,x_n) \in \mathcal{X}^n\)</span>,</li>
</ol>
<span class="math display">\[\begin{align}
\mathbb{E}\Bigg[ \Big|P(X) - \sum_{i=1}^n f(x_i)\Big|\Bigg] &amp;= O\Bigg( \frac{1}{\epsilon}\ln \frac{n}{\delta}\Bigg)
\end{align}\]</span>
<p><strong>Each user sends <span class="math inline">\(\Theta(\epsilon\sqrt{n})\)</span> messages.</strong></p>
<p>Later work by the same authors handles the case when the upper bound on the numbers is much higher.</p>
<div class="theorem">

</div>
<h2 id="binary-sums">Binary Sums</h2>
<p>Below we describe the classic randomised response algorithm</p>
<div class="algorithm">
<p><strong>Local Randomiser:</strong> Inputs: <span class="math inline">\(x_i \in \{ 0, 1\}\)</span>, <span class="math inline">\(\lambda \in \{ 1, \dots, n-1\}\)</span></p>
<p>Let <span class="math inline">\(p=\frac{\lambda}{n}\)</span>, each user tosses a coin with probability of heads being <span class="math inline">\(p\)</span></p>
<ul>
<li>If heads: <span class="math inline">\(y_i = Bernoulli(1/2)\)</span></li>
<li>If tails: <span class="math inline">\(y_i = x_i\)</span></li>
</ul>
<p>return <span class="math inline">\(y_i\)</span></p>
<p><strong>Analyser:</strong> Inputs: <span class="math inline">\(Y=\{ y_1, \dots, y_n \}\)</span>, <span class="math inline">\(\lambda\)</span> from local randomiser</p>
<p>Output: <span class="math inline">\(\frac{n}{n - \lambda}\Big(\sum_{i=1}^n y_i - \frac{\lambda}{2}\Big)\)</span></p>
</div>
<button type="button" class="btn btn-info" data-toggle="collapse" data-target="#rrUnbiased">
The above estimator is an unbiased estimator of <span class="math inline">\(\sum_{i=1}^n x_i\)</span>
</button>
<div id="rrUnbiased" class="collapse">
<span class="math display">\[\begin{align*}
\mathbb{P}[y_i=1] &amp;= (1-p)\mathbb{P}[x_i=1] + \frac{p}{2} \\
\mathbb{P}[x_i=1] &amp;= \frac{\mathbb{P}[y_i=1] - \frac{p}{2}}{1-p} \\
\frac{\sum_{i=1}^n x_i}{n} &amp;= \frac{\frac{\sum_{i=1}^n y_i}{n} - p\frac{1}{2}}{1-p} \tag{1}\label{MLE}\\
\sum_{i=1}^n x_i &amp;= \frac{n}{n - \lambda}\Big(\sum_{i=1}^n y_i - \frac{\lambda}{2}\Big)
\end{align*}\]</span>
<p><span class="math inline">\(\ref{MLE}:\)</span> Whre the MLE of <span class="math inline">\(\mathbb{P}[y_i=1]\)</span> is <span class="math inline">\(\frac{\sum_{i=1}^n y_i}{n}\)</span></p>
<p>Thus in expectation the output of the algorithm gives us the Expected value of sum of binary values.</p>
</div>
<h3 id="an-alternate-way-to-describe-the-above-algorithm">An alternate way to describe the above algorithm</h3>
<p>An equivalent version of the local algorithm can be described by the following algorithm. <strong>Assume each person has been made anonymous, so we can use shuffle privacy and central privacy synonymously</strong> In this section we compare the Cheu algorithm to Sample and threshold by Graham. Throughout this paper, we derive the missing pieces for Sample and threshold analysis.</p>
<div class="algorithm">
<p><strong>Analyser</strong> Inputs: <span class="math inline">\(Y=\{ x_1, \dots, x_n \}\)</span>, Same <span class="math inline">\(\lambda\)</span> from local randomiser</p>
<p>Let <span class="math inline">\(p=\frac{\lambda}{n}\)</span></p>
<p><span class="math inline">\(s \leftarrow Binomial(n, p)\)</span></p>
<p>Define <span class="math inline">\(\mathcal{H}_s = \{ H \subseteq \left\lfloor n \right\rfloor | |H| = s\}\)</span></p>
<p>In words <span class="math inline">\(\mathcal{H}_s\)</span> is a set of sets. Each set inside <span class="math inline">\(\mathcal{H}_s\)</span> is a way to select <span class="math inline">\(s\)</span> elements from <span class="math inline">\(\left\lfloor n \right\rfloor\)</span>. There are <span class="math inline">\({n \choose s}\)</span> ways of doing this.</p>
<p>Select <span class="math inline">\(H \sim Uniform(\mathcal{H}_S)\)</span></p>
<div class="intuition">
<p><span class="math inline">\(H\)</span> represents a set obtained by Poisson sampling with parameter <span class="math inline">\(p\)</span>, where Poisson sampling is described how Graham describes it in his Sample and Threshold paper.</p>
</div>
<p>Final output: <span class="math inline">\(\sum_{i \notin H}y_i + Bin(s, \frac{1}{2})\)</span></p>
<div class="intuition">
<p>In the sample and threshold regime the final output is.</p>
<p>Final output: <span class="math inline">\(\sum_{i \notin H}y_i + \sum_{i \notin H} 0\)</span></p>
<p>We get exactly forced randomised response <a href="https://en.wikipedia.org/wiki/Randomized_response">see first example of wikipedia page</a></p>
<p><span class="math inline">\(\sum_{i \notin H}y_i + \sum_{i \notin H} 1\)</span></p>
</div>
</div>
<h1 id="references">References</h1>
<!-- [1]: https://arxiv.org/pdf/2109.13158.pdf "Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message"
1. [Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message](https://arxiv.org/pdf/2109.13158.pdf)

[2]: https://arxiv.org/pdf/1908.11358.pdf  "On the power of multiple anonymous messages"
2. [On the power of multiple anonymous messages](https://arxiv.org/pdf/1908.11358.pdf)
 -->
</div>
<div id="footer">
  “Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”― Richard Feynmann
</div>
</body>
</html>
