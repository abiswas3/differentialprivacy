<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../blog.css">
  <script src="../../code/es5/tex-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/code/bootstrap.min.css">
  <script src="/code/jquery.min.js"></script>
  <script src="/code/bootstrap.min.js"></script>
  
  
  <ul class="bar">
    <li class="barli"><a class="active" href="/">Home</a></li>
    <li class="barli"><a href="/differentialprivacy/index.html">Notes on Differential Privacy</a></li>  
    <!-- <li class="barli"><a href="https://abiswas3.github.io/differentialprivacy.github.io">Notes on Differential Privacy</a></li> -->
  </ul>
</head>
<body>

<div class="container">

<h1 id="distributed-population-mean-estimation">Distributed Population mean estimation</h1>
<p>Not to be confused with mean estimation of a statistical distribution. Those papers are similar but the end goal is different and so are the guarantees.</p>
<h2 id="summary-of-the-state-of-affairs">Summary of the state of affairs</h2>
<p><span class="math inline">\(N\)</span> users have values <span class="math inline">\(x_i \in \{0,1\}\)</span>, where <span class="math inline">\(i=\{1, 2, \dots, N\}\)</span> For now we focus on binary values, later we will extend to integers. The goal is to approximate the the mean/sum of the population privately, without each user sending their values to some central aggregator. The problem was first considered by [<a href="https://arxiv.org/pdf/1811.12469.pdf" title="Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity">5</a>]. They stated it as – <code>What can we say about central DP guarantees of a protocol that satisfies</code><span class="math inline">\(\epsilon\)</span>-local DP? Since then, there have been two approaches to this problem – additive noise mechanism (think laplace) and sampling based mechanisms (randomised response).</p>
<p>This makes sense as we are looking at local privacy from a central perspective. So we considered the most basic central model and moved it to the a local setting or considered the most basic local model and moved it to understand a central setting.</p>
<p><img src="./pngs/mechanisms.jpeg" width="80%"></img></p>
<p>The noise distributions for additive mechanisms tend to be infinitely divisible distributions. This allows us to locally perturb the private values while still doing the analysis from a central perspective. The table below summarises the important contributions to the problem of population mean estimation.</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
<tr>
<th class="tg-0lax">
</th>
<th class="tg-1wig">
Contribution
</th>
<th class="tg-1wig">
Venue
</th>
<th class="tg-1wig">
Authors
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Any permutation invariant algorithm satisfying <span class="math inline">\(\epsilon\)</span>-local differential privacy will satisfy <span class="math inline">\((O(\frac{\epsilon}{\sqrt{n}}\sqrt{log1/δ}), δ)\)</span>-central differential privacy. Or more simply by using lazy notation, <span class="math inline">\(\epsilon = O_{\epsilon, \delta}(\frac{1}{\sqrt{n}})\)</span></span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms (2019)</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Erlingsson, Ulfar and Feldman, Vitaly and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Thakurta, Abhradeep</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
<ul>
<li><p>Amplification bound generalizes the results by Erlingsson et al. to a wider range of parameters, and provides a whole family of methods to analyze privacy amplification in the shuffle model.</p></li>
<li><p>A new lower bound for the accuracy of private protocols for summation of real numbers in the shuffle model. There are lot of fancy constants but the asymptotic complexity for the lower bound is still <span class="math inline">\(O_{\epsilon, \delta}(\frac{1}{\sqrt{n}})\)</span>, which</p></li>
<li>Provide an optimal single message protocol for summation of real numbers in the shuffle model. This is based on randomised response as well
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Annual International Cryptology Conference (2019)</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Balle, Borja and Bell, James and Gasc{'o}n, Adria and Nissim, Kobbi</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
Distributed mean estimation using randomised response- more of the same of the above two really. This analysis was for integers and binary values. The above paper is an improvement on this
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Annual International Conference on the Theory and Applications of Cryptographic Techniques (2019)</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Cheu, Albert and Smith, Adam and Ullman, Jonathan and Zeber, David and Zhilyaev, Maxim</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
Distributed mean estimation using binomial noise
</td>
<td class="tg-0lax">
<span style="font-style:normal">Information-Theoretic Cryptography, </span><br><span style="font-style:normal">ITC 2020</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Balcer, Victor and Cheu, Alber</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
Distributed mean estimation using correlated noise - binary population<br>
</td>
<td class="tg-0lax">
ICML 2020
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">B Ghazi, R Kumar, P Manurangsi, R Pagh</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
Distributed mean estimation using correlated noise - integer valued population.
</td>
<td class="tg-0lax">
ICML 2021
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Badih Ghazi, Noah Zeger Golowich, Shanmugasundaram Ravikumar, Pasin Manurangsi, Ameya Avinash Velingker, Rasmus Pagh</span>
</td>
</tr>
</tbody>
</table></li>
</ul>
<p>Not mentioned in above table but of relevance in Sample and Threshold mechanism proposed by Graham.</p>
<h2 id="questions">Questions</h2>
<h4 id="these-questions-were-posed-on-21-08-2021">These questions were posed on 21-08-2021</h4>
<pre><code>I had in mind the algorithm that outputs the mean of the sampled items, without censoring.  However, I think some bounds need to be placed on the size of the items being reported.  Otherwise, we have the &#39;billionaire&#39;s problem&#39;: looking at the mean reveals whether or not there was a billioniaire in the sample, which in turn reveals whether or not they were in the input.</code></pre>
This was posed initially when we were discussing the sample and threshold problem. The question was, whether there was a way to use this mechanism to report sums and means of populations instead of reporting succint histograms of the population. I was able to show that reporting the sum of the histogram/mean is DP but this is not very satisfying, as by creating a histogram we have created buckets and brought in rounding error.
<div class="intuition">
<strong>TODO: </strong> How did Ghazi et al 2021 ICML paper get around this problem for non binary valued integers? I cannot remember at the top of my head
</div>
<h4 id="these-questions-were-posed-on-23-11-2021.">These questions were posed on 23-11-2021.</h4>
<pre><code>Among all these options, are there any strict dominances, i.e., is there any method that has an asymptotically better guarantee than another across all epsilon and n choices?  I think probably not, but not sure yet.</code></pre>
<pre><code>Otherwise, how best can we compare these?  Maybe plotting the shape of the distributions for a few different settings will help to compare them better. </code></pre>
<pre><code>Can we characterize which parameter regimes work best for each method (dense/sparse data, high/low populations size, epsilon value etc.)?</code></pre>
<button type="button" class="btn btn-info" data-toggle="collapse" data-target="#comparingAlgs">
How to compare additive mechanisms with multiplicative mechanisms - new conjectures
</button>
<div id="comparingAlgs" class="collapse">
<p><strong>Last update: 30-01-2022</strong></p>
<p>All these methods work by flipping a coin with probability of heads <span class="math inline">\(p\)</span>, <span class="math inline">\(n\)</span> times. The exact value of <span class="math inline">\(p\)</span> is given by <span class="math inline">\((\epsilon, \delta)\)</span>. Let’s ignore <span class="math inline">\((\epsilon, \delta)\)</span> and fix <span class="math inline">\(p\)</span> and see how they compare. We expect additive noise mechanisms to have best accuracy, followed by sampling and randomised response to have the worst accuracy. Intuition provided below:</p>
<h4 id="inutuition-as-to-why">Inutuition as to why</h4>
<p>Consider a single person <span class="math inline">\(x_i \in \{0, 1\}\)</span>. They have a local randomiser <span class="math inline">\(L \in \{\text{Add}, \text{Sample}, \text{RR}\}\)</span>. They perturb their input with each local randomiser and send their output to 3 different analysers <span class="math inline">\(A_{\text{Add}}, A_{\text{Sample}}\)</span> and <span class="math inline">\(A_{\text{RR}}\)</span>. Let <span class="math inline">\(y_i = L(x_i)\)</span>, the value after perturbation.</p>
<p><u> Consider <span class="math inline">\(A_{\text{Add}}\)</span> </u>.</p>
<p>If the analyser sees a <span class="math inline">\(2\)</span> or a <span class="math inline">\(0\)</span>. They know for sure that <span class="math inline">\(x_i\)</span> was <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> respectively. Only when <span class="math inline">\(y_i = 1\)</span> is when the anlyser has to reconstruct the true input value.</p>
<p><u> Consider <span class="math inline">\(A_{\text{Sample}}\)</span> </u>.</p>
<p>If the analyser sees a <span class="math inline">\(1\)</span>. They know for sure that <span class="math inline">\(x_i\)</span> was <span class="math inline">\(1\)</span>. If they see a <span class="math inline">\(y_i=0\)</span>, they must re-construct the input.</p>
<p><u> Consider <span class="math inline">\(A_{\text{RR}}\)</span></u></p>
<p>The analyser never knows <span class="math inline">\(x_i\)</span> with certainty for any value of <span class="math inline">\(y_i\)</span>. This is the most private thereby the Analyser has to reconstruct <span class="math inline">\(x_i\)</span> all the time. Note this agrees with all the results cited above. In the separating local and shuffle privacy paper [<a href="https://privacytools.seas.harvard.edu/publications/separating-local-shuffled-differential-privacy-histograms" title="Seperating Local and Shuffle privacy via histograms">6</a>], the authors show that additive noise outperforms their previous randomised response model.</p>
<h4 id="a-little-experiment-to-verify-this-conjecture">A little experiment to verify this conjecture</h4>
<p>Before trying to formally develop theory for this intuition, we ran some experiments to verify if our conjecture was true.</p>
<p>Let <span class="math inline">\(n\)</span> be the number of users. Let <span class="math inline">\(k=\sum_{i=1}^n x_i\)</span>, the number of users that voted yes. Define error as <span class="math inline">\(|k - f(X)|\)</span> where <span class="math inline">\(f(X)\)</span> is the estimated number of yes votes by the algorithm. Ignore <span class="math inline">\((\epsilon, \delta)\)</span> for now and just focus on a fixed <span class="math inline">\(p\)</span>.</p>
<p>We find that Randomised Response is always worse than the other methods, but as expected Additive noise works better when the data is sparse i.e. fraction of people that say yes is low. Sampling on the other hand does better when this fraction is higher. Too see why, refer to the document below where we derive the variance of the sampling estimator.</p>
</div>
<pre><code>Are there any approaches that are analytically or empirically the same (we tried to argue that binomial noise addition was equivalent to sampling, but this dod not go through). </code></pre>
<p>This is still unresolved but I have a new idea using arithmetic circuits, that I would like to discuss <br></p>
<pre><code>Can we make single proof of a general result that holds for distributions with some property, and then specialize it for each of these different approaches?  Or is that already done in the prior work?</code></pre>
<p>The paper power of multiple anonymous messages does this for all additive methods. They define smooth distributions or distributions with a certain hockey stick divergence. Then they show that the central additive mehcanism is good for all such distributions. For the distributed case, if your distribution is infinitely divisible, you win. If we can connect multiplicative noise distributions to these distributions it would give us a general proof under their framework. We can also try to come up with something new. <br></p>
<h4 id="these-questions-were-posed-on-07-12-2021">These questions were posed on 07-12-2021</h4>
<p>I had sent a pdf writing out explicitly the variance and mean of the sampling estimator based on binomial coefficients.</p>
<pre><code>Ari: Assuming communication complexity is not an issue and we can talk to all n people, then when n is large, additive noise mechanisms 
usually always win. This makes sense to me, I’ve added my reasoning in the writeup.

Graham: For the paper, we were able to show DP by considering the explicit calculation for cases differing by 1, but I wonder if there is a
more general way to show this.</code></pre>
<pre><code>Ari: When n is medium sized, additive mechanism do not give privacy but sampling and thresholding does. It is because only a very small p 
is required to keep the variance of the estimator small.

Graham: There&#39;s a bigger communication aspect that I only focused on recently: for the histogram problem, it should be possible for the 
sampling approach to send only a constant amount of information per client. Other noise-addition approches require much more communication, 
proportional to the size of the histogram per client.  This can be a big difference.</code></pre>
<pre><code>Ari: If communication complexity matters (and you discuss this in the paper); you can still sample using m + O(sqrt(m)) people which is 
much lesser than n. So this method is once again useful. Though the error does not shrink as fast.
I can see why the connection between additive noise and sampling is so tempting; particularly if there was a connection then we
 could reduce communication complexity by  m + O(sqrt(m)).

Graham: This is an intersting niche.</code></pre>
<pre><code>Additional remarks:
Thanks for the notes.  The calculations look correct, based on binonial distribution.  I expect there is some nice statistical tricks to manipulate the pmf further.  Do you think there is any chance for a closed form for the variance, or is it best to leave it to numerical evaluation?

Reducing the communication in this way is essentially being able to suppress messages in the noise setting, i.e., having noise that leads us to output zero most of the time.  This may not be possible without effectively including sampling in the method, and incurring the corresponding error from sampling.</code></pre>
<h2 id="references-and-links-exchanged">References and Links exchanged</h2>
<ol type="1">
<li><a href="../DiscreteLaplace/paper.pdf">UNIVERSALLY UTILITY-MAXIMIZING PRIVACY MECHANISMS</a></li>
</ol>
<ol start="2" type="1">
<li><a href="https://arxiv.org/pdf/1903.02837.pdf">The privacy blanket of the shuffle model</a></li>
</ol>
<ol start="3" type="1">
<li><a href="https://arxiv.org/pdf/1103.2626.pdf">Distributed Private Data Analysis: On Simultaneously Solving How and What</a></li>
</ol>
<ol start="4" type="1">
<li><a href="https://mathoverflow.net/questions/213221/what-is-a-two-sided-geometric-distribution">Distinction between one sided and 2 sided Geometric distributions</a></li>
</ol>
<p>[Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity](https://arxiv.org/pdf/1811.12469.pdf “Amplification by Shuffling:)</p>
<p><a href="https://privacytools.seas.harvard.edu/publications/separating-local-shuffled-differential-privacy-histograms">Seperating Local and Shuffle privacy via histograms</a></p>
<h2 id="links">Links</h2>
<p>Streaming paper including voting: https://arxiv.org/abs/1603.00213</p>
<p>Local differential privacy for voting/ranking https://arxiv.org/pdf/1908.04920.pdf (there may be some follow up)</p>
<p>Your notes on privacy: https://abiswas3.github.io/differentialprivacy/</p>
<p>Bonus: Mean estimation with low communication: https://arxiv.org/abs/2010.02331 (privacy follows by applying randomized response to the bit that is sent)</p>
</div>
<div id="footer">
  “Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”― Richard Feynmann
</div>
</body>
</html>
