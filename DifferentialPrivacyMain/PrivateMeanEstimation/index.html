<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../blog.css">
  <script src="../../code/es5/tex-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/code/bootstrap.min.css">
  <script src="/code/jquery.min.js"></script>
  <script src="/code/bootstrap.min.js"></script>
  
  
  <ul class="bar">
    <li class="barli"><a class="active" href="/">Home</a></li>
    <li class="barli"><a href="/differentialprivacy/index.html">My notes</a></li>  
    <li class="barli"><a href="./..">Back</a></li>    
    <!-- <li class="barli"><a href="https://abiswas3.github.io/differentialprivacy.github.io">Notes on Differential Privacy</a></li> -->
  </ul>
</head>
<body>

<div class="container">

<h1 id="distributed-population-mean-estimation">Distributed Population mean estimation</h1>
<p><strong>Disclaimer for self:</strong> Not to be confused with parameter (mean) estimation of a finite population drawm from an unknown statistical distribution. Although similar techniques are used, the guarantees are very different.</p>
<h2 id="summary-of-the-state-of-affairs">Summary of the state of affairs</h2>
<p><span class="math inline">\(N\)</span> users have values <span class="math inline">\(x_i \in \{0,1\}\)</span>, where <span class="math inline">\(i=\{1, 2, \dots, N\}\)</span>. We focus on binary values, later we will extend to integers or bounded reals. The goal is to approximate the the mean/sum of the population privately, without each user sending their values to some central aggregator. The problem was first considered by [<a href="https://arxiv.org/pdf/1811.12469.pdf" title="Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity">5</a>]. They stated it as – <code>What can we say about central DP guarantees of a protocol that satisfies</code><span class="math inline">\(\epsilon\)</span>-local DP? In the original paper, they show that by looking at Randomised Response from a central perspective the error could be drastically be redeuced. Since then, there have been a series of papers on this topic but they can all be described by one of two approaches – additive noise mechanism (the aggregator adds noise from some distribution and outputs statistics) and sampling based mechanisms (mulitplicative noise - the analyser supresses some values completely and replaces them with random noise while revealing a subset only). The figure below illustrates the general idea</p>
<p><img src="./pngs/mechanisms.jpeg" width="80%"></img></p>
<p>To make this ordeal distributed, the noise distributions for additive mechanisms tend to be infinitely divisible distributions. This allows us to locally perturb the private values while still doing the analysis from a central perspective. The table below summarises the important contributions to the problem of population mean estimation.</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
<tr>
<th class="tg-0lax">
</th>
<th class="tg-1wig">
Contribution
</th>
<th class="tg-1wig">
Venue
</th>
<th class="tg-1wig">
Authors
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Any permutation invariant algorithm satisfying <span class="math inline">\(\epsilon\)</span>-local differential privacy will satisfy <span class="math inline">\((O(\frac{\epsilon}{\sqrt{n}}\sqrt{log1/δ}), δ)\)</span>-central differential privacy. Or more simply by using lazy notation, <span class="math inline">\(\epsilon = O_{\epsilon, \delta}(\frac{1}{\sqrt{n}})\)</span></span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms (2019) [<a href="https://arxiv.org/pdf/1811.12469.pdf" title="Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity">8</a>]</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Erlingsson, Ulfar and Feldman, Vitaly and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Thakurta, Abhradeep</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
<ul>
<li>Amplification bound generalizes the results by Erlingsson et al. to a wider range of parameters, and provides a whole family of methods to analyze privacy amplification in the shuffle model.</li>
<li>A new lower bound for the accuracy of private protocols for summation of real numbers in the shuffle model. There are lot of fancy constants but the asymptotic complexity for the lower bound is still <span class="math inline">\(O_{\epsilon, \delta}(\frac{1}{\sqrt{n}})\)</span>, which</li>
<li>Provide an optimal single message protocol for summation of real numbers in the shuffle model. This is based on randomised response as well</li>
</ul>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Annual International Cryptology Conference (2019) [<a href="https://arxiv.org/pdf/1903.02837.pdf" title="The privacy blanket of the shuffle model">2</a>]</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Balle, Borja and Bell, James and Gasc{'o}n, Adria and Nissim, Kobbi</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
Distributed mean estimation using randomised response- more of the same of the above two really. This analysis was for integers and binary values. The above paper is an improvement on this. <a href="../CheuShuffleRR/">My writeup</a>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Annual International Conference on the Theory and Applications of Cryptographic Techniques (2019) [<a href="https://arxiv.org/pdf/1811.12469.pdf" title="Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity">5</a>]</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Cheu, Albert and Smith, Adam and Ullman, Jonathan and Zeber, David and Zhilyaev, Maxim</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
<ul>
<li>Develop a protocol protocol that estimates histograms with error independent of the domain size. This implies an arbitrarily large gap in sample complexity between the shuffled and local models when considering <span class="math inline">\((\epsilon, \delta)\)</span> privacy. Most of the heavy lifting for this protocol was done here [<a href="https://arxiv.org/abs/1908.11358" title="On the Power of Multiple Anonymous Messages">7</a>] by the Google group.</li>
<li>If we want pure privacy from a single message, shuffle privacy and local privacy are equivalent <a href="../ShufflePrivacy/">My writeup</a></li>
</ul>
</td>
<td class="tg-0lax">
<span style="font-style:normal">Information-Theoretic Cryptography, </span><br><span style="font-style:normal">ITC 2020 [<a href="https://privacytools.seas.harvard.edu/publications/separating-local-shuffled-differential-privacy-histograms" title="Seperating Local and Shuffle privacy via histograms">6</a>]</span>
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Balcer, Victor and Cheu, Alber</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
<ul>
<li>Really a generalisation of the their previous paper [<a href="https://arxiv.org/abs/1908.11358" title="On the Power of Multiple Anonymous Messages">7</a>]. They extend that by finding infinitely divisible distributions that area also smooth.</li>
<li>A second contribution is the correlated noise mechanism that allows them to re-produce the discrete laplace mechanism in the distributed setting.
<div class="intuition">
In practice this doesn’t make that much of a difference. Borja has a paper that already shows the Gaussian mechanism is not that far off the laplace mechanism. The Binomial is the discrete cousin of the Gaussian. Adding Binomial noise is simpler.</li>
</ul>
<p><a href="../ShuffleSumBinaryRasmus">My writeup</a></p>
</div>
</td>
<td class="tg-0lax">
ICML 2020 [<a href="https://arxiv.org/abs/2106.04247" title="Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead">9</a>]
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">B Ghazi, R Kumar, P Manurangsi, R Pagh</span>
</td>
</tr>
<tr>
<td class="tg-0lax">
</td>
<td class="tg-0lax">
Extend the above method for summing bounded integers to get central privacy error rates for shuffled privacy. <strong>I have not read the proofs in detail yet</strong>
</td>
<td class="tg-0lax">
ICML 2021
</td>
<td class="tg-0lax">
<span style="font-weight:400;font-style:normal">Badih Ghazi, Noah Zeger Golowich, Shanmugasundaram Ravikumar, Pasin Manurangsi, Ameya Avinash Velingker, Rasmus Pagh</span>
</td>
</tr>
</tbody>
</table>
<p><br> <strong>Not mentioned in above table but of relevance is Sample and Threshold mechanism proposed by Graham.</strong></p>
<h2 id="questionsopen-problems">Questions/Open Problems</h2>
<p>Graham and I have been working on understanding how multiplicative noise and additive noise are related over the last few months. In the next few sections, we summarise some of the open problems in this area.</p>
<h4 id="section">21-08-2021</h4>
<pre><code>I had in mind the algorithm that outputs the mean of the sampled items, without censoring.  However, I think some bounds need to be placed on the size of the items being reported.  Otherwise, we have the &#39;billionaire&#39;s problem&#39;: looking at the mean reveals whether or not there was a billioniaire in the sample, which in turn reveals whether or not they were in the input.</code></pre>
<p>This was posed initially when we were discussing the sample and threshold problem. The question was, whether there was a way to use this mechanism to report sums and means of populations instead of reporting succint histograms of the population. I was able to show that reporting the sum of the histogram/mean is DP but this is not very satisfying, as by creating a histogram we have created buckets and brought in rounding error.<strong>We never settled this. Recent discoveries might make this worth attending to.</strong></p>
<h4 id="section-1">23-11-2021</h4>
<pre><code>Among all these options, are there any strict dominances, i.e., is there any method that has an asymptotically better guarantee than another across all epsilon and n choices?  I think probably not, but not sure yet.</code></pre>
<pre><code>Otherwise, how best can we compare these?  Maybe plotting the shape of the distributions for a few different settings will help to compare them better. </code></pre>
<pre><code>Can we characterize which parameter regimes work best for each method (dense/sparse data, high/low populations size, epsilon value etc.)?</code></pre>
<button type="button" class="btn btn-info" data-toggle="collapse" data-target="#comparingAlgs">
How to compare additive mechanisms with multiplicative mechanisms - new conjectures
</button>
<div class=collapse id=comparingAlgs>

<p><strong>Last update: 30-01-2022</strong></p>
<p>All these methods work by flipping a coin with probability of heads <span class="math inline">\(p\)</span>, <span class="math inline">\(n\)</span> times. The exact value of <span class="math inline">\(p\)</span> is given by <span class="math inline">\((\epsilon, \delta)\)</span>. Let’s ignore <span class="math inline">\((\epsilon, \delta)\)</span> and fix <span class="math inline">\(p\)</span> and see how they compare. We expect additive noise mechanisms to have best accuracy, followed by sampling and randomised response to have the worst accuracy. Intuition provided below:</p>
<h4 id="inutuition-as-to-why">Inutuition as to why</h4>
<p>Consider a single person <span class="math inline">\(x_i \in \{0, 1\}\)</span>. They have a local randomiser <span class="math inline">\(L \in \{\text{Add}, \text{Sample}, \text{RR}\}\)</span>. They perturb their input with each local randomiser and send their output to 3 different analysers <span class="math inline">\(A_{\text{Add}}, A_{\text{Sample}}\)</span> and <span class="math inline">\(A_{\text{RR}}\)</span>. Let <span class="math inline">\(y_i = L(x_i)\)</span>, the value after perturbation.</p>
<p><u> Consider <span class="math inline">\(A_{\text{Add}}\)</span> </u>.</p>
<p>If the analyser sees a <span class="math inline">\(2\)</span> or a <span class="math inline">\(0\)</span>. They know for sure that <span class="math inline">\(x_i\)</span> was <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> respectively. Only when <span class="math inline">\(y_i = 1\)</span> is when the anlyser has to reconstruct the true input value.</p>
<p><u> Consider <span class="math inline">\(A_{\text{Sample}}\)</span> </u>.</p>
<p>If the analyser sees a <span class="math inline">\(1\)</span>. They know for sure that <span class="math inline">\(x_i\)</span> was <span class="math inline">\(1\)</span>. If they see a <span class="math inline">\(y_i=0\)</span>, they must re-construct the input.</p>
<p><u> Consider <span class="math inline">\(A_{\text{RR}}\)</span></u></p>
<p>The analyser never knows <span class="math inline">\(x_i\)</span> with certainty for any value of <span class="math inline">\(y_i\)</span>. This is the most private thereby the Analyser has to reconstruct <span class="math inline">\(x_i\)</span> all the time. Note this agrees with all the results cited above. In the separating local and shuffle privacy paper [<a href="https://privacytools.seas.harvard.edu/publications/separating-local-shuffled-differential-privacy-histograms" title="Seperating Local and Shuffle privacy via histograms">6</a>], the authors show that additive noise outperforms their previous randomised response model.</p>
<h4 id="a-little-experiment-to-verify-this-conjecture">A little experiment to verify this conjecture</h4>
<p>Before trying to formally develop theory for this intuition, we ran some experiments to verify if our conjecture was true.</p>
<p>Let <span class="math inline">\(n\)</span> be the number of users. Let <span class="math inline">\(k=\sum_{i=1}^n x_i\)</span>, the number of users that voted yes. Define error as <span class="math inline">\(|k - f(X)|\)</span> where <span class="math inline">\(f(X)\)</span> is the estimated number of yes votes by the algorithm. Ignore <span class="math inline">\((\epsilon, \delta)\)</span> for now and just focus on a fixed <span class="math inline">\(p\)</span>.</p>
<p><strong>NOTE: </strong> <span class="math inline">\(n\)</span> is too small for additive mechanisms to provide privacy here.</p>
<embed type="text/html" src="./Comparison.html" width="800" height="600">
<p>We find that Randomised Response is always worse than the other methods, but as expected Additive noise works better when the data is sparse i.e. fraction of people that say yes is low. Sampling on the other hand does better when this fraction is higher. Too see why, refer to the document below where we derive the variance of the sampling estimator.</p>
<p><strong>IMPORATANT:</strong> This would also imply that for any given <span class="math inline">\((\epsilon, \delta)\)</span> and <span class="math inline">\(n\)</span> it would be easiest to satisfy privacy with Randomise Response, then shuffle privacy and then additive noise. This is exactly what we found when analysing the <span class="math inline">\((\epsilon, \delta)\)</span> relationships with <span class="math inline">\(p\)</span>. We have already discussed that additive mechanisms need a very large value for <span class="math inline">\(n\)</span> before they can give us privacy. Randomised Response does not have any requirements for <span class="math inline">\(n\)</span> as it is a locally private algorithm. Sampling does not place restriction on <span class="math inline">\(n\)</span> but puts restrictions on the value of <span class="math inline">\(k\)</span>. To be useful, it states a lower bound on <span class="math inline">\(k\)</span>.</p>
</div>
<p><br></p>
<pre><code>Are there any approaches that are analytically or empirically the same (we tried to argue that binomial noise addition was equivalent to sampling, but this dod not go through). 

</code></pre>
<button type="button" class="btn btn-info" data-toggle="collapse" data-target="#multAddConnection">
New Results: Connecting Additive and Multiplicative
</button>
<div id="multAddConnection" class="collapse">
<p>Our previous attempt failed as <strong>OR</strong> is not the same is as <strong>Addition</strong> but i found a new way to connect them.</p>
<p>All that SNIPs reading with circuits and graduate school pre-reading of computational theory made me draw up the boolean circuits that express sampling and additive mechanisms as circuits. The imporant difference is Sampling is a 1 bit protocol whereas addition is a 2 bit protocol. This extra bit allows for more wiggle room in general. The second thing to note is that if we truncated the addition operation and kept the MSB then the two protocols are equivalent.</p>
<p>I re-state two imporant theorems (1) By Cheu that says if you are 1 message only, the not local implies not shuffle. This means that the sampling method is not shuffle DP. (2) Randomised Response is the best you can do in local and shuffle DP if you are restricted to 1 bit/message communications. Thus Sample privacy is a paradigm that is less private than 1 bit shuffle but more accurate based on all experiments shown in the previous step.</p>
<p>See the writeup below. Aplogies for small illegible handwriting.</p>
<p><img src="./pngs/circuits.jpeg" width="80%"></img> <br> <img src="./pngs/circuits2.jpeg" width="80%"></img></p>
</div>
<p><br></p>
<pre><code>Can we make single proof of a general result that holds for distributions with some property, and then specialize it for each of these different approaches?  Or is that already done in the prior work?</code></pre>
<p>The paper power of multiple anonymous messages does this for all additive methods. They define smooth distributions or distributions with a certain hockey stick divergence. Then they show that the central additive mehcanism is good for all such distributions. For the distributed case, if your distribution is infinitely divisible, you win. If we can connect multiplicative noise distributions to these distributions it would give us a general proof under their framework. We can also try to come up with something new. <strong>I am optimistic there is a better and cleaner proof</strong>. <br></p>
<h4 id="section-2">07-12-2021</h4>
<p>I had sent a pdf writing out explicitly the variance and mean of the sampling estimator based on binomial coefficients.</p>
<pre><code>Ari: Assuming communication complexity is not an issue and we can talk to all n people, then when n is large, additive noise mechanisms 
usually always win. This makes sense to me, I’ve added my reasoning in the writeup.

Graham: For the paper, we were able to show DP by considering the explicit calculation for cases differing by 1, but I wonder if there is a
more general way to show this.</code></pre>
<pre><code>Ari: When n is medium sized, additive mechanism do not give privacy but sampling and thresholding does. It is because only a very small p 
is required to keep the variance of the estimator small.

Graham: There&#39;s a bigger communication aspect that I only focused on recently: for the histogram problem, it should be possible for the 
sampling approach to send only a constant amount of information per client. Other noise-addition approches require much more communication, 
proportional to the size of the histogram per client.  This can be a big difference.</code></pre>
<pre><code>Ari: If communication complexity matters (and you discuss this in the paper); you can still sample using m + O(sqrt(m)) people which is 
much lesser than n. So this method is once again useful. Though the error does not shrink as fast.
I can see why the connection between additive noise and sampling is so tempting; particularly if there was a connection then we
 could reduce communication complexity by  m + O(sqrt(m)).

Graham: This is an intersting niche.</code></pre>
<p><strong>A lot of these questions are answered in the previous section</strong>.</p>
<h4 id="additional-remarks">Additional remarks</h4>
<p>I wrote a <a href="./Variance_of_Sampling.pdf">short note</a> to derive the variance of sampling estimator. <strong>TODO</strong> The final form is quite ugly- full of sums of binomial coefficients, I have to still upper bound in standard form still.</p>
<pre><code>Thanks for the notes.  The calculations look correct, based on binonial distribution.  I expect there is some nice statistical tricks to manipulate the pmf further.  Do you think there is any chance for a closed form for the variance, or is it best to leave it to numerical evaluation?

Reducing the communication in this way is essentially being able to suppress messages in the noise setting, i.e., having noise that leads us to output zero most of the time.  This may not be possible without effectively including sampling in the method, and incurring the corresponding error from sampling.</code></pre>
<h4 id="section-3">2022-02-02</h4>
<pre><code>Is it accurate to call randomized response &#39;multiplicative&#39;?  RR can take an input 0 --&gt; 1 in the output, so it&#39;s not only doing multiplications.

Let&#39;s talk more about the Boolean circuit representation.  There is some work on DP + circuits, but I think it is approaching from a different direction -- more from a computational complexity perspective
(e.g., https://privacytools.seas.harvard.edu/publications/complexity-verifying-loop-free-programs-differentially-private )

The fact that sample+threshold sits outside of shuffle is not very surprising -- the thresholding is not something we can achieve.  But it would be easy to generalize the shuffler to perform the threshold step, creating a new model.

I&#39;m more interested in understanding other forms of distributed noise generation.  Is there anything proposed in the shuffle model that can&#39;t be easily simulated via secure aggregation?  The tech companies seem to be moving more in the direction of secure aggregation rather than shuffling implementations.  The Bell et al. secagg paper mentions that they can effectively perform shuffling within secure aggregation.
</code></pre>
<h2 id="references">References</h2>
<ol type="1">
<li><a href="../DiscreteLaplace/paper.pdf">UNIVERSALLY UTILITY-MAXIMIZING PRIVACY MECHANISMS</a></li>
</ol>
<ol start="2" type="1">
<li><a href="https://arxiv.org/pdf/1903.02837.pdf">The privacy blanket of the shuffle model</a></li>
</ol>
<ol start="3" type="1">
<li><a href="https://arxiv.org/pdf/1103.2626.pdf">Distributed Private Data Analysis: On Simultaneously Solving How and What</a></li>
</ol>
<ol start="4" type="1">
<li><a href="https://mathoverflow.net/questions/213221/what-is-a-two-sided-geometric-distribution">Distinction between one sided and 2 sided Geometric distributions</a></li>
</ol>
<ol start="5" type="1">
<li><a href="https://arxiv.org/pdf/1811.12469.pdf">Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity</a></li>
</ol>
<ol start="6" type="1">
<li><a href="https://privacytools.seas.harvard.edu/publications/separating-local-shuffled-differential-privacy-histograms">Seperating Local and Shuffle privacy via histograms</a></li>
</ol>
<ol start="7" type="1">
<li><a href="https://arxiv.org/abs/1908.11358">On the Power of Multiple Anonymous Messages</a></li>
</ol>
<ol start="8" type="1">
<li><a href="https://arxiv.org/pdf/1811.12469.pdf">Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity</a></li>
</ol>
<ol start="9" type="1">
<li><a href="https://arxiv.org/abs/2106.04247">Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead</a></li>
</ol>
<h2 id="other-links-shared-in-the-time-frame">Other Links shared in the time frame</h2>
<ul>
<li><a href="https://arxiv.org/abs/1603.00213">Streaming paper including voting</a></li>
<li><a href="https://arxiv.org/pdf/1908.04920.pdf">Local differential privacy for voting/ranking</a>: This was not very useful. This was just Laplace Mechanism with very little insight into something new.</li>
<li><a href="https://arxiv.org/abs/2010.02331">Bonus- Mean estimation with low communication:</a> privacy follows by applying randomized response to the bit that is sent</li>
</ul>
</div>
<div id="footer">
  “Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”― Richard Feynmann
</div>
</body>
</html>
