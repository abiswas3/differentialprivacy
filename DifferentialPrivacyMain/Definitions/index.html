<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Differential Privacy why?</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../blog.css">
  <script src="../../code/es5/tex-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="/code/bootstrap.min.css">
  <script src="/code/jquery.min.js"></script>
  <script src="/code/bootstrap.min.js"></script>
  
  
  <ul class="bar">
    <li class="barli"><a class="active" href="/">Home</a></li>
    <li class="barli"><a href="/differentialprivacy/index.html">My notes</a></li>  
    <li class="barli"><a href="./..">Back</a></li>    
    <!-- <li class="barli"><a href="https://abiswas3.github.io/differentialprivacy.github.io">Notes on Differential Privacy</a></li> -->
  </ul>
</head>
<body>
<header>
<h1 class="title">Differential Privacy why?</h1>
</header>

<div class="container">
<p><strong>TODO: Re-write this whole thing again based on Vadhans notes</strong></p>
<h1 id="what-does-it-mean-to-be-differentially-private">What does it mean to be differentially private</h1>
<p><strong>Draw picture for model</strong></p>
<h2 id="counting-queries">Counting queries</h2>
<p>The definition of differential privacy requires that no individual’s data has much effect on what an adversary sees. If we have two datasets <span class="math inline">\(x \sim x&#39;\)</span> of size <span class="math inline">\(n\)</span> records that differ by just 1 record, then the outputs of <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(q(x&#39;)\)</span>, released via mechanism <span class="math inline">\(M\)</span> should have a <em>similar</em> distribution.</p>
<div class="lemma">
<p>For <span class="math inline">\(\epsilon \geq 0\)</span>, we say mechanism <span class="math inline">\(M\)</span> is <span class="math inline">\(\epsilon\)</span> differentially private if <strong>for all</strong> neighbouring datasets <span class="math inline">\(x \sim x&#39;\)</span>, and for every query <span class="math inline">\(q \in Q\)</span> and <span class="math inline">\(\forall T \subseteq Y\)</span> we have</p>
<span class="math display">\[\begin{align*}
\mathbb{P}\Big[M(x, q) \in T\Big] \leq (1 + \epsilon)\mathbb{P}\Big[M(x&#39;, q) \in T\Big]
\end{align*}\]</span>
</div>
<p><br> For a small value <span class="math inline">\(\epsilon\)</span>, it is saying that <span class="math inline">\(|\mathbb{P}\Big[M(x, q) \in T\Big] - \mathbb{P}\Big[M(x\, q) \in T\Big]|\)</span> is close to 0. By Taylors Theorem <span class="math inline">\(1 + \epsilon \leq e^{\epsilon}\)</span> <span class="math inline">\(\forall \epsilon \geq 0\)</span>, so the defintion that is used in practice is as exponents are easier to manipulate.</p>
<div class="lemma">
<p>For <span class="math inline">\(\epsilon \geq 0\)</span>, we say mechanism <span class="math inline">\(M\)</span> is <span class="math inline">\(\epsilon\)</span> differentially private if <strong>for all</strong> neighbouring datasets <span class="math inline">\(x \sim x&#39;\)</span>, and for every query <span class="math inline">\(q \in Q\)</span> and <span class="math inline">\(\forall T \subseteq Y\)</span> we have</p>
<span class="math display">\[\begin{align*}
\mathbb{P}\Big[M(x, q) \in T\Big] &amp;\leq e^{\epsilon}\mathbb{P}\Big[M(x&#39;, q) \in T\Big]
\end{align*}\]</span>
</div>
<p><br></p>
<div class="important">
<p><strong>From Page 6 if [<a href="https://arxiv.org/pdf/1908.11358.pdf" title="On the power of multiple anonymous messages">1</a>]</strong></p>
<p>Here we typically take <span class="math inline">\(\epsilon\)</span> as small, but non-negligible (not cryptographically small); for example, a small constant, such as <span class="math inline">\(\epsilon= 0.1\)</span>. Smaller = 0.1 provides better privacy, but as we will see, the definition is no longer useful when <span class="math inline">\(\epsilon &lt; 1/n\)</span>.</p>
</div>
<h2 id="why-not-use-statistical-distances">Why not use statistical distances ?</h2>
<p>The first defintion looks a lot like some sort sort of statistical distance. One could ask why not use a defintion based on stastical distance such as total variation distance and the answer is two fold:</p>
<p><strong>Total Variation Distance between two distributions</strong></p>
<span class="math display">\[\begin{align*}
SD_q(M(x), M(x&#39;)) := \text{max}_{T \subseteq Y} \Bigg| \mathbb{P}\Big[M(x, q) \in T\Big] - \mathbb{P}\Big[M(x&#39;, q) \in T\Big]\Bigg| \leq \delta
\end{align*}\]</span>
<p>Firstly, the current defintion of differential privacy implies statistical privacy, but the converse is not true. Assume we have <span class="math inline">\(\epsilon\)</span>-DP. Setting <span class="math inline">\(\delta = 1 - e^{-\epsilon} \leq \epsilon\)</span> we get bounded statistical distance:</p>
<span class="math display">\[\begin{align*}
\text{max}_{T \subseteq Y} \Bigg| \mathbb{P}\Big[M(x, q) \in T\Big] - \mathbb{P}\Big[M(x&#39;, q) \in T\Big]\Bigg| &amp;\leq 1 - e^{-\epsilon} \\
&amp;\leq \epsilon + 1\tag{1}\label{1} \\
&amp;\leq e^{\epsilon} \tag{2}\label{2} \\
\end{align*}\]</span>
<p><span class="math inline">\(\ref{1}\)</span> and <span class="math inline">\(\ref{2}\)</span> come from Taylors theorem, see <a href="../../GraduateSchoolCourses/TCS_Toolkit-Ryan_ODonell/AsymptoticsAndGaussians/">course notes from Ryan O Donnels class for more details</a>. Thus this defintion is a stronger statement of closeness.</p>
<p>Secondly, depending on the setting of <span class="math inline">\(\delta\)</span>, the defintion is either too private or not private at all. See below:</p>
<ul>
<li><p><strong>Too private <span class="math inline">\(\delta \leq \frac{1}{2n}\)</span></strong>: There is some hybrid argument I do not fully understand, but one can show that output of the mechanism is independent of dataset – making the mechanism useless for finding properties about data. Read more about <a href="https://eprint.iacr.org/2021/088.pdf">hybrid arguments here</a></p></li>
<li><p><strong>Not private at all <span class="math inline">\(\delta \geq \frac{1}{2n}\)</span>: </strong> Assume two neighbouring datasets <span class="math inline">\(x \sim x&#39;\)</span> and <span class="math inline">\(y \in \mathcal{X}\)</span> is the record missing in <span class="math inline">\(x&#39;\)</span>. Consider the mechanism that picks a random row from the dataset and outputs it with probability <span class="math inline">\(\frac{1}{2}\)</span>. Then <span class="math inline">\(\mathbb{P}\Big[M(x, q) = y\Big] = \frac{1}{2n}\)</span> and <span class="math inline">\(\mathbb{P}\Big[M(x&#39;, q) = y\Big] = 0\)</span>. Therefore, <span class="math inline">\(SD(M(x, q), M(x&#39;, q)) \geq \Bigg| \mathbb{P}\Big[M(x, q) =y\Big] - \mathbb{P}\Big[M(x&#39;, q) = y \Big]\Bigg| = \frac{1}{2n}\)</span>, <strong>which satisfies our requirement of statistical privacy</strong>. However, we just revealed a dataset in public.</p></li>
</ul>
<h2 id="relaxations">Relaxations</h2>
<h2 id="how-do-we-get-pure-differential-privacy">How do we get pure differential privacy</h2>
<h3 id="laplace-mechanism">Laplace mechanism</h3>
<h3 id="randomised-response">Randomised Response</h3>
<h2 id="robustness-to-post-processing">Robustness to Post Processing</h2>
<h2 id="group-privacy">Group privacy</h2>
<h2 id="composition-lemmas">Composition Lemmas</h2>
<h3 id="basic-composition">Basic composition</h3>
<h3 id="advanced-composition">Advanced Composition</h3>
<h3 id="arbitrary-many-counting-queries">Arbitrary many counting queries</h3>
<!-- With probablity 1 - $\delta$. 

$$ \frac{ \P{M(x) \in T}{x}{X^n}}{ \P{M(x') \in T}{x'}{X^n}} \leq e^{\epsilon}$$

Usually $\delta = o(1/n)$ is accepted as workable or efficient. This
is saying if datasets are neighbouring, central DP gurantees that the
algorithm outputs are concentrated near each other with high probability.
 -->
<!-- ## Local Models  {#definition-localModel}

First introduced in [[2][2]]

A protocol P in the (non-interactive) local model consists of two
randomized algorithms:

* A randomiser $\textit{R}: X \rightarrow Y$ that takes as input a
  single user’s data and outputs a message.
* An analyser $\textit{A}: Y^* \rightarrow Z$ that takes as input all
  user messages and computes the output of the protocol.

We denote the protocol $\textit{P} = (\textit{R}, \textit{A})$. We
assume that the number of users n is public and available to both
$\textit{R}$ and $\textit{A}$. Let $x \in X^n$. The evaluation of the
protocol \textit{P} on input $x$

$$P(x) = (A \circ R)(x) = A\Big(R(x_1), \dots, R(x_n)\Big)$$




## Local Differential Privacy {#definition-localDP}

A local protocol $\localP$ satisfies $\epsDelta$-differential privacy
for n users if its randomizer $R : X \rightarrow Y$ is
$\epsDelta$-differentially private (for datasets of size one). In
other words, the output of the randomiser for any two inputs must be
concentrated with high probability 1 - $\delta$

$$ \frac{ \P{R(x) \in T}{x}{X}}{ \P{R(x') \in T}{x'}{X}} \leq
e^{\epsilon}$$ for all subsets $T \subseteq Y$

## Shuffled Model {#definition-shuffleModel}

A protocol $P$ in the shuffled model consists of three randomized
algorithms. The first and last algorithm is identical to the local
model. There is a middle shuffle phase. Formally,

* A randomiser $\textit{R}: X \rightarrow Y$ that takes as input a
  single user’s data and outputs a message.
* A shuffler $S : Y^* \rightarrow Y^*$. that concatenates all message
  vectors and then applies a uniformly random permutation to (the
  order of) the concatenated vector. For example, when there are three
  users each sending two messages, there are 6! permutations and all
  are equally likely to be the output of the shuffler.**NOTE: There is
  no special shuffling algorithm. It's always uniformly random on all
  permutations**
* An analyser $\textit{A}: Y^* \rightarrow Z$ that takes as input the
  shuffled messages and computes the output of the protocol.

The evaluation of the protocol $P=(R,A)$ on input $x$ is

$$P(x) = (A \circ S \circ R)(x) = A\Big(S\big[R(x_1), \dots, R(x_n)\big]\Big)$$

## Shuffle Privacy

A shuffled protocol $P = (R,A)$ satisfies $\epsDelta$-differential
privacy for $n$ users if the algorithm $(S \circ R) : X^n \rightarrow
Y^∗$ is $\epsDelta$-differentially private.

This is saying, for two neighbouring datasets $x \sim x'$, the
probability of seeing the same permutation after shuffling the outputs
of all the local randomisers is concentrated with high probability 1 -
$\delta$.

The hope is the following: Because we have this shuffle phase, perhaps
we can add a little less noise to the local randomisers thereby
getting a little bit more accuracy. 


## L1 Sensitivity 

The $l_1$-sensitivity of a function $f: \N^{|X|} \rightarrow \R^k$ is defined as 

$$\Delta f = \text{max}_{|| x - x' ||_1 \leq 1} || f(x) - f(x')||_1$$

for any $x, x' \in \N^{|X|}$. 


Simply put, for neighbouring datasets$x, x'$, what is the maximum this function can differ by. High
sensitive functions are not good differential privacy, as by changing
just one input we see big differences in outputs -- thereby providing
information about the unique input to the function.


## Laplace Mechanism {#theorem-lapIsPrivate}

** FIX TYPOS in notation of proof. Using same variables over and over** 
Given $f: \N^{|X|} \rightarrow \R^k$, then the laplace mechanism is
defined as 

$$M_L\Big(x, f(.), \epsilon \Big) = f(x) + (Y_1, \dots, Y_k)$$ where
$Y_i \sim Lap(\frac{\Delta(f)}{\epsilon})$. **The laplace mechanism
preserves pure differential privacy**

<button type="button" 
class="btn btn-info" 
data-toggle="collapse" 
data-target="#laplaceMech">Proof</button>
<div class=collapse id=laplaceMech>

Let $f_x(z)$ be the density function for $M_L\Big(x, f(.), \epsilon
\Big)$. We want to show that $\frac{f_x(z)}{f_x'(z)} \leq
e^{\epsilon}$ where $x, x'$ differ by one element only.

We have $z_i= f_i(x) + Y_i$, the only randomness is from $Y_i$, thus
we have just scaled the noise $Y_i$ by a constant. Thus $f_x(z)$ is
just a laplace random variable with mean $f_i(x)$.


\begin{align*}
\frac{f_x(z)}{f_x'(z)} &= \prod_{i=1}^k \frac{\frac{exp\{-|f_i(x) - z_i|\epsilon}{\Delta f}\}}{\frac{exp\{-|f_i(x') - z_i|\epsilon}{\Delta f}\}} \\
&= \prod_{i=1}^k exp \{ \frac{\epsilon}{\Delta f}\Big(|f_i(x') - z_i| - |z_i - f_i(x)| \Big)\} \\
&\leq \prod_{i=1}^k exp \{ \frac{\epsilon}{\Delta f}|f_i(x') - f_i(x)| \} \label{eq1}\tag{1}\\
&= exp \{ \frac{\epsilon}{\Delta f}\sum_{i=1}^k|f_i(x') - f_i(x)| \}\\
&= exp \{ \frac{\epsilon}{\Delta f}||f(x') - f(x)||_1 \} \\
&\leq exp \{ \frac{\epsilon}{\Delta f} \Delta f \} \label{eq2}\tag{2} \\
&= exp(\epsilon)
\end{align*}

$\ref{eq1}$: Triangle inequality: $|A - C| \leq |A - B| + |B - C|$

$\ref{eq2}$: Definitition of $l_1$-sensitivity
</div>

#### Binary sums or real valued sums (counting and histogram queries)

Consider the example where each member of the popuplation $x \in X$
holds a value in $\{ 0, 1\}$ or in some bounded set of integers. We
want to estimate the sum of values for the population. Then the
sensitivity for this sum query function is either 1 for binary values
or the suprenum value of the population of integers. To report sums
differentially privately, the noise needed is $O(\frac{\Delta
f}{\epsilon})$. In the case of two neighbouring histograms, the
sensitivity is given by the maximum difference between two cell
values.

### Accuracy of the Laplace Mechanism {#theorem-lapAcc}

$\alpha, \beta$ accuracy

<button type="button" 
class="btn btn-info" 
data-toggle="collapse" 
data-target="#lapAcc">Proof</button>
<div class=collapse id=lapAcc>

</div>


### Exponential noise 

Similar bounds to Laplace

\begin{align*}
\mathbb{P}\Big[ \frac{M(D) \in T}{M(D') \in T}\Big] &= e^{-\lambda(|f(X) - f(X'))} \\
&\geq e^{-\lambda(\Delta f)} \\
&\geq e^{-\epsilon}
\end{align*}

where $\epsilon = \lambda \times \Delta f$ and $\lambda$ is the rate parameter of the noise distribution. The variance of the distribution is $\frac{1}{\lambda^2} = O(\frac{1}{\epsilon^2})$

### An experiment validating the theoretical bound

Writing a computer program to validate the above theory. The
concentration inequality says that if I performed the laplace
mechanism once then - the likelihood one of my dimensions is off by
$\log \frac{k}{\beta}\frac{\Delta f}{\epsilon}$ is $\beta$. Put in
otherwords, say I did the laplace mechanism a gazillion times on my
output. Only $\beta$ fraction of times will I get a bad event. A bad
event is when the laplace mechanism leads to an output very far from
the truth. So as an experiment, I generated many noisy but private
outputs and observed how often the infinity norm went outside the
boundary of the theorem. Was it more than theory claimed or less. It
turns out when the number trials is large i.e. empirical means are
close to actual means, the bound is quite tight.

 <embed type="text/html" src="code/sample.html" width="800" height="600"> 
 
### Why do smooth distributions with fatty tails work



## Exponential Mechanism

**This is a theoretical introduction of exponential mechanism. A
practical application of such an idea can be found in making password
lists private.** <div class=intuition>While final result of [the
password list problem](../passwordLists/) is relatively uninteresting
to me, the mathematics in the papers is quite interesting to me.</div>


Consider case where output of a function is very sensitive. Outputs
like voting counts or auction prices. Adding noise to these values
could damaging results. Take for example predicting the winner of the
Balon D'or football prize. Say 30 voters voted for Cristiano Ronaldo,
and 29 voters voted for Lionel Messi. The fair action of concern is to
award Ronaldo the prize. No one really cares how close we were in
approximating the number votes in terms of infinity norm to the actual
value, as long as the right player wins. In cases like this, adding
noise is not a viable solution as there is a significant chance that
we get a terrible outcome.


What happens when an algorithm outputs values in an arbitrary range
$R$ that is not $\R^k$ for $k \geq 1$. This is what the exponential
mechanism was designed for. Given some arbitrary range $R$, the
exponential mechanism is defined with respect to some utility function
$u: \N^{|X|} \times R \rightarrow \R$.

**Do not really understand** we care only about the sensitivity of u
with respect to its database argument; it can be arbitrarily sensitive
in its range argument. <div class=intuition>What they mean is that the
$r$ is somewhat of a free parameter. Given two neighbouring datasets
$x, x'$ the sensitivity is the maximum difference in their utility:
considering all values of the range those database arguments map
to.</div>


$$\S{u} := \max_{r \in R}\max_{|| x - x'||_1 \leq 1} | u(x,r) - u(x',
r)|$$


### The mechanism {#definition--expMech}

The basic idea is given some utility function, for a given $x \in
\N^{|X|}$, we want to the mechanism to output $r \in R \propto exp
\Big( \frac{\epsilon}{2\S{u}}u(x, r)\Big)$

### Exponential Mechanism is purely private

The exponential mechanism satisfies $(\epsilon, 0)$ privacy. Thus
there is never a chance of a bad event in the privacy sphere.

<button type="button" 
class="btn btn-info" 
data-toggle="collapse" 
data-target="#expMechIsPrivate">Proof</button>
<div class=collapse id=expMechIsPrivate>


</div>

 -->
<h2 id="important-papers">Important Papers</h2>
<ol type="1">
<li><a href="https://arxiv.org/pdf/1908.11358.pdf">On the power of multiple anonymous messages</a></li>
</ol>
<ol start="2" type="1">
<li><a href="https://arxiv.org/pdf/0803.0924.pdf">What can we learn privately</a></li>
</ol>
</div>
<div id="footer">
  “Study hard what interests you the most in the most undisciplined, irreverent and original manner possible.”― Richard Feynmann
</div>
</body>
</html>
